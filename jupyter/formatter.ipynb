{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install influxdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not included installs, but useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from influxdb import DataFrameClient\n",
    "import json\n",
    "# import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "purge=False\n",
    "# purge=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_annoations=False\n",
    "# process_annoations=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_macro=False\n",
    "process_macro=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_out=False\n",
    "write_out=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_overwrite=False\n",
    "# full_overwrite=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_csv('df0.csv').set_index('date')\n",
    "df0.index=pd.to_datetime(df0.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bets=json.loads(open('bets.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity=pd.read_csv('severity.csv').set_index('date')\n",
    "severity.index=pd.to_datetime(severity.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility=pd.read_csv('mobility.csv').set_index('date')\n",
    "mobility.index=pd.to_datetime(mobility.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_mini=pd.read_csv('mobility_mini.csv').set_index('date')\n",
    "mobility_mini.index=pd.to_datetime(mobility_mini.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'admin'\n",
    "password = open('auth/influxa.txt','r').read()\n",
    "host='influxdb'\n",
    "port=8086\n",
    "dbname='base'\n",
    "dbname_long='long'\n",
    "protocol = 'line' #'json'\n",
    "client = DataFrameClient(host, port, user, password, dbname)\n",
    "client_long = DataFrameClient(host, port, user, password, dbname_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if purge:\n",
    "    client.drop_database(dbname)\n",
    "    client.drop_retention_policy(dbname)\n",
    "    client.create_database(dbname)\n",
    "    client.create_retention_policy(dbname, '600d', 1, default=True)\n",
    "    client_long.drop_database(dbname_long)\n",
    "    client_long.drop_retention_policy(dbname_long)\n",
    "    client_long.create_database(dbname_long)\n",
    "    client_long.create_retention_policy(dbname_long, '6000d', 1, default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlipath='../html/'\n",
    "htmlepath='//myserv.er/'\n",
    "htmlepath_other='//mybackupserv.er/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles={'HU':\"Magyar\",'RO':'Română','EN':'English'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtitles={titles[t]:t for t in titles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "grafana = \"http://grafana:3000/\"\n",
    "headers = {\n",
    "    'Authorization': 'Bearer '+open('auth/grafana.txt','r').read(),\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(grafana+'api/folders', headers=headers)\n",
    "folders=json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_id=[f['id'] for f in folders if f['title']=='My Grafana Folder'][0] #General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(grafana+'api/search?folderIds='+str(folder_id), headers=headers)\n",
    "dashs=json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids={rtitles[d['title']]:d['uid'] for d in dashs if d['title'] in rtitles}\n",
    "iids={rtitles[d['title']]:d['id'] for d in dashs if d['title'] in rtitles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_light={rtitles[d['title'].split(' Light')[0]]:d['uid'] for d in dashs if 'Light' in d['title']}\n",
    "iids_light={rtitles[d['title'].split(' Light')[0]]:d['id'] for d in dashs if 'Light' in d['title']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EN': 'en', 'HU': 'hu', 'RO': 'ro'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=['HU','RO','EN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://docs.google.com/spreadsheets/d/'+open('auth/sheet.txt','r').read()+'/gviz/tq?tqx=out:csv&sheet='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_szotar():\n",
    "    sheet='szotar'\n",
    "    columns=languages+[i+'_description' for i in languages]+[i+'_source' for i in languages]\n",
    "    df=pd.read_csv(url+sheet)\n",
    "    df=df[['ID','UI']+columns]\n",
    "    sheet='minidashboard'\n",
    "    df2=pd.read_csv(url+sheet)\n",
    "    df2=df2[['ID','UI']+columns]\n",
    "    df=pd.concat([df,df2])\n",
    "    szotardf=df.set_index('ID')[columns]\n",
    "    szotar=df.set_index('ID').T.to_dict()\n",
    "    szotarHU=df.set_index('HU',drop=False).T.to_dict()\n",
    "    szotarRO=df.set_index('RO',drop=False).T.to_dict()\n",
    "    szotarEN=df.set_index('EN',drop=False).T.to_dict()\n",
    "    return szotardf,szotar,szotarHU,szotarRO,szotarEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_time_conflicts(series):\n",
    "    ds={}\n",
    "    ts=[]\n",
    "    for d in series:\n",
    "        if d not in ds:\n",
    "            ds[d]=pd.to_datetime(d)\n",
    "            t=(ds[d])\n",
    "        else:\n",
    "            ds[d]=ds[d]+pd.to_timedelta('193m')\n",
    "            t=(ds[d])\n",
    "        ts.append(t)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-77652dbc15c9>:12: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  szotarHU=df.set_index('HU',drop=False).T.to_dict()\n",
      "<ipython-input-36-77652dbc15c9>:13: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  szotarRO=df.set_index('RO',drop=False).T.to_dict()\n",
      "<ipython-input-36-77652dbc15c9>:14: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  szotarEN=df.set_index('EN',drop=False).T.to_dict()\n"
     ]
    }
   ],
   "source": [
    "szotardf,szotar,szotarHU,szotarRO,szotarEN=get_szotar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "utc=pytz.UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push2influx(df,measurement,field_columns,tag_columns,shift=False,dbclient=client,wo=write_out,fo=full_overwrite,daily=True):\n",
    "    if wo:\n",
    "        df=df.sort_index()\n",
    "        # df.index = utc.localize(df.index) \n",
    "        df.index=df.index.tz_localize('GMT')#.tz_convert('Europe/Bucharest')\n",
    "        if shift:\n",
    "            df.index+=pd.to_timedelta('12h')\n",
    "        if fo: \n",
    "            print('Purging',measurement,'...')\n",
    "            dbclient.query('DROP MEASUREMENT '+measurement)\n",
    "        else:\n",
    "            latest=dbclient.query('SELECT * FROM '+measurement+' GROUP BY \"1d\" ORDER BY DESC LIMIT 1')\n",
    "            if latest:\n",
    "                lat=latest[list(latest.keys())[0]].index[0]\n",
    "                if daily: lat+=pd.to_timedelta('1d')\n",
    "                df=df[lat:]\n",
    "                print('Slicing',measurement,'from',lat,'...')\n",
    "            else:\n",
    "                print('No data in db for',measurement,'...')\n",
    "        time.sleep(3)\n",
    "        print('Writing to',measurement,'...')\n",
    "        bsize=5000\n",
    "        bwait=2\n",
    "        print(len(df),'data points will be written in',len(df)/bsize,'batches.')\n",
    "        print('Expected query running time is:',int((len(df)/bsize)*bwait*1.1)+3,'seconds.')\n",
    "        for i in range(int(len(df)/bsize)+1):\n",
    "            r=range(i*bsize,min(len(df),(i+1)*bsize))\n",
    "            dc=df.iloc[r]\n",
    "            print('Writing batch',i+1,'...')\n",
    "            dbclient.write_points(dc, measurement, protocol=protocol,\n",
    "                                field_columns=field_columns,\n",
    "                                tag_columns=[])\n",
    "            time.sleep(bwait)\n",
    "        time.sleep(3)\n",
    "        print('Done!')\n",
    "    else:\n",
    "        print('Write-out not enabled. Skipping...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DateLaZi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new method from DateLaZi\n",
    "dlz=requests.get('https://datelazi.ro/latestData.json').content\n",
    "dlz=json.loads(dlz)\n",
    "dlz['historicalData'][dlz['currentDayStats']['parsedOnString']]=dlz['currentDayStats']\n",
    "dlzs=[]\n",
    "dlz_counties=[]\n",
    "for date in dlz['historicalData']:\n",
    "    d={}\n",
    "    mdate=date.replace('2018-11-07','2020-11-07').replace('2020-01-05','2021-01-05').replace('2020-01-07','2021-01-07')\n",
    "    d['date']=mdate\n",
    "    d['cases']=dlz['historicalData'][date]['numberInfected']\n",
    "    d['heals']=dlz['historicalData'][date]['numberCured']\n",
    "    d['deaths']=dlz['historicalData'][date]['numberDeceased']\n",
    "    if 'vaccines' in dlz['historicalData'][date]:\n",
    "        if dlz['historicalData'][date]['vaccines']:\n",
    "            for v in dlz['historicalData'][date]['vaccines']:\n",
    "                for k in ['total_administered','immunized']:\n",
    "                    k2=k.replace('immunized','total_immunized')\n",
    "                    if k2 not in d: d[k2]=0\n",
    "                    d[k2]+=dlz['historicalData'][date]['vaccines'][v][k]\n",
    "                    d[k2+'_'+v]=dlz['historicalData'][date]['vaccines'][v][k]\n",
    "    if 'countyInfectionsNumbers' in dlz['historicalData'][date]:\n",
    "        counties=dlz['historicalData'][date]['countyInfectionsNumbers']\n",
    "        if counties:\n",
    "            for county in counties:\n",
    "                dummy={'date':mdate,county:dlz['historicalData'][date]['countyInfectionsNumbers'][county]}\n",
    "                if county=='PH':\n",
    "                    if date=='2020-12-01':\n",
    "                        dummy[county]=19395\n",
    "                dlz_counties.append(dummy)\n",
    "    dlzs.append(d)\n",
    "dl=pd.DataFrame(dlzs).set_index('date').sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in dl.columns[3:]:\n",
    "    dl[c]=dl[c].cumsum()\n",
    "dl2=dl.diff()\n",
    "dl2.columns=['case','heal','death']+[i.replace('total_','') for i in dl2.columns[3:]]\n",
    "dl['active']=dl['cases']-dl['heals']-dl['deaths']\n",
    "dl=dl.join(dl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-06 2021-04-06\n"
     ]
    }
   ],
   "source": [
    "latest=dl.index[-1]\n",
    "now=str(pd.to_datetime(\"now\"))[:10]\n",
    "print(latest,now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests from `graphs.ro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-06\n"
     ]
    }
   ],
   "source": [
    "r=requests.get('https://www.graphs.ro/json.php')\n",
    "graphs_ro=json.loads(r.content)['covid_romania']\n",
    "g=pd.DataFrame(graphs_ro)\n",
    "# g['date']=pd.to_datetime(g['reporting_date'])\n",
    "g['date']=g['reporting_date']\n",
    "g=g.set_index('date')\n",
    "g['tests']=g['total_tests']\n",
    "g['test']=g['new_tests_today']\n",
    "latest2=g.index[0]\n",
    "print(latest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-06\n"
     ]
    }
   ],
   "source": [
    "latest=str(min(pd.to_datetime(latest),pd.to_datetime(latest2)))[:10]\n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl=dl.join(g[['tests','test']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaccines from `graphs.ro`\n",
    "\n",
    "_deprecated_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r=requests.get('https://www.graphs.ro/vaccinare_json.php')#?data_date=2021-01-05\n",
    "# vc=json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vc=pd.DataFrame(vc['covid_romania_vaccination']).set_index('reporting_date')\n",
    "# vc=vc[['total_first_dose']]\n",
    "# vc.columns=['vaccines']\n",
    "# # vc.index=pd.to_datetime(vc.index)\n",
    "# vc.index.name='date'\n",
    "# latest3=vc.index[0]\n",
    "# print(latest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=str(min(pd.to_datetime(latest),pd.to_datetime(latest3)))[:10]\n",
    "# print(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl=dl.join(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='governance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(url+sheet)\n",
    "# dl=pd.concat([df.set_index('date').sort_index()[:'2020-03-17'][dl.columns[:-1]],\n",
    "#           dl['2020-03-18':]])\n",
    "df=df.set_index('date').sort_index(ascending='True').join(dl,lsuffix='_g')#[20:] #[:-1] vaccines nincs a governance-ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill from local, if missing in DateLaZi\n",
    "for c in ['active','cases','heals','deaths','case','heal','death']:\n",
    "    df[c]=df[c].fillna(df[c+'_g'].astype(str).str.replace(',','')).replace('nan',np.nan)\n",
    "df=df.dropna(how='all',axis=0).dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df14=df[['cases']].reset_index().join(df[['cases']][14:].reset_index(),lsuffix='1')\n",
    "df14['case14']=df14['cases'].astype(float)-df14['cases1'].astype(float)\n",
    "df=df.join(df14.set_index('date')['case14'].dropna().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index=pd.to_datetime(df.index)\n",
    "df0=df.copy()\n",
    "#df['date']=pd.to_datetime(df['date'])\n",
    "#df=df[df.columns[:13]].set_index('date')\n",
    "vaccine_totals=[i for i in list(dl2.columns[3:])+list(dl.columns[3:len(dl2.columns)]) if 'total_' in i]\n",
    "df=pd.DataFrame(df[['active','cases','heals','deaths','case14']+\\\n",
    "                  vaccine_totals].stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value']\n",
    "df['value']=df['value'].astype(str).str.replace(',','').astype(float).astype(int)\n",
    "df=df.join(szotardf,on='type')\n",
    "df=pd.DataFrame(df.reset_index().set_index(['date','type','value']).stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value','lang','langtype']\n",
    "current=df[df['type']=='case14']['value'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.to_csv('df0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0=pd.read_csv('df0.csv').set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Run till here to test Grafana only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.replace(0,np.nan).dropna()\n",
    "df1=df1[df1['type'].isin(['active', 'cases', 'heals', 'deaths', 'case14'])]\n",
    "df2=df.replace(0,np.nan).dropna()\n",
    "df2=df2[~(df2['type'].isin(['active', 'cases', 'heals', 'deaths', 'case14']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['type','lang','langtype']\n",
    "measurement='governance1'\n",
    "# push2influx(df1,measurement,field_columns,tag_columns,fo=True)\n",
    "push2influx(df1,measurement,field_columns,tag_columns)\n",
    "measurement='vaccine1'\n",
    "# push2influx(df2,measurement,field_columns,tag_columns,fo=True)\n",
    "push2influx(df2,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(url+sheet)\n",
    "df=df0.copy()\n",
    "\n",
    "# df['date']=pd.to_datetime(df['date'])\n",
    "# df=df[df.columns[:13]].set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['case/test']=100*df['case'].astype(str).str.replace(',','').astype(float)/\\\n",
    "    df['test'].astype(str).str.replace(',','').astype(float)\n",
    "df['death_rate']=100*df['death'].astype(str).str.replace(',','').astype(float)/\\\n",
    "    df['case'].astype(str).str.replace(',','').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(df[['death','heal','case','case/test','death_rate']].stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value']\n",
    "df['value']=df['value'].astype(str).str.replace(',','').astype(float)\n",
    "df=df.join(szotardf,on='type')\n",
    "df=pd.DataFrame(df.reset_index().set_index(['date','type','value']).stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value','lang','langtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['type','lang','langtype']\n",
    "measurement='governance2'\n",
    "# push2influx(df.replace(0,np.nan).dropna(),measurement,field_columns,tag_columns,fo=True)\n",
    "push2influx(df.replace(0,np.nan).dropna(),measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='C19RFMC'\n",
    "df=pd.read_csv(url+sheet).drop('Day',axis=1)\n",
    "df['date']=pd.to_datetime(df['Date'])\n",
    "df=df.drop('Date',axis=1).set_index('date')\n",
    "# latestw='2020-11-08'\n",
    "latestw=df[['Low 95 line']].dropna().index[0]\n",
    "df.loc[latestw+pd.to_timedelta('-1d')]=df.loc[latestw+pd.to_timedelta('-1d')].bfill().ffill()\n",
    "df1=df.loc[:latestw+pd.to_timedelta('-1d')][['Cases (fact & forecast)']]\n",
    "df1.columns=['M1']\n",
    "df2=df.loc[latestw+pd.to_timedelta('-1d'):][['Cases (fact & forecast)']]\n",
    "df2.columns=['M2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.join(df1).join(df2)\n",
    "df=df.stack().reset_index().set_index('date')#.loc[latest:]\n",
    "df.columns=['type','value']\n",
    "df['value']=df['value'].astype(str).str.replace(',','').str.replace('%','').astype(float).astype(int)\n",
    "df=df[~(df['type'].isin(['Cases (fact & forecast)', 'Low 80 line','High 80 line',]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type']=df['type'].replace({'M1':'Fact', 'Low 95 line':'-95%', 'Low 60 line':'-60%', \n",
    "                               'High 60 line':'+60%', 'High 95 line':'+95%',\n",
    "       'M2':'Forecast'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging forecast ...\n",
      "Writing to forecast ...\n",
      "499 data points will be written in 0.0998 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['type']\n",
    "measurement='forecast'\n",
    "push2influx(df,measurement,field_columns,tag_columns,fo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='countries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(url+sheet)\n",
    "df['date']=pd.to_datetime(df['date'])\n",
    "df=df[df.columns[:13]].set_index('date')\n",
    "df=df.stack().reset_index().set_index('date')\n",
    "df.columns=['type','value']\n",
    "df['value']=df['value'].astype(str).str.replace(',','').str.replace('%','').astype(float).astype(int)\n",
    "df=df.join(szotardf,on='type')\n",
    "df=pd.DataFrame(df.reset_index().set_index(['date','type','value']).stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value','lang','langtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_columns=['value']\n",
    "# tag_columns=['type','lang','langtype']\n",
    "# measurement='global1'\n",
    "# push2influx(df,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='global' #annotations\n",
    "df=pd.read_csv(url+sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=pd.DataFrame(szotardf.stack())\n",
    "ds.columns=['country']\n",
    "ds=ds.reset_index()\n",
    "ds['index2']=ds['ID']+ds['level_1']\n",
    "dc=pd.DataFrame(df.set_index(['Dátum','tag1','Link'])[languages].stack()).reset_index()\n",
    "dc['index1']=dc['tag1']+dc['level_3']\n",
    "dc=dc.set_index('index1').join(ds.set_index('index2')).set_index('Dátum')[[0,'country','Link','level_1']]\n",
    "dc.columns=['desc','country','link','lang']\n",
    "dc.index=pd.to_datetime(dc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing global2 from 2021-02-07 00:00:00+00:00 ...\n",
      "Writing to global2 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['desc','country','link']\n",
    "tag_columns=['lang']\n",
    "measurement='global2'\n",
    "push2influx(dc,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries=['Brazil','France','Germany','Italy','Romania','Spain','US']\n",
    "# columns=['br','fr','de','it','ro','es','us']\n",
    "countries=['Brazil','France','Germany','Italy','Romania','US','United Kingdom']\n",
    "columns=['br','fr','de','it','ro','us','gb']\n",
    "eus=['Austria','Belgium','Bulgaria','Croatia','Cyprus', 'Czechia', 'Denmark','Estonia',\n",
    "    'Finland', 'France','Germany','Greece','Hungary','Ireland','Italy','Latvia','Lithuania','Luxembourg',\n",
    "    'Malta','Netherlands','Poland', 'Portugal', 'Romania','Slovakia','Slovenia','Spain','Sweden']\n",
    "eus+=['United Kingdom','Andorra','Armenia','Azerbaijan','Belarus','Bosnia and Herzegovina','Georgia', \n",
    "     'Holy See','Iceland','Liechtenstein', 'Moldova','Montenegro','North Macedonia',\n",
    "       'Norway','Russia','San Marino','Switzerland','Turkey','Ukraine','Kosovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs=['https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv',\n",
    "'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv',\n",
    "'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\n",
      "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\n",
      "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\n"
     ]
    }
   ],
   "source": [
    "dfs=[]\n",
    "x=4\n",
    "#sheets=['countries_auto_cases','countries_auto_heals','countries_auto_deaths']\n",
    "for i in range(len(csvs)):\n",
    "    csv=csvs[i]\n",
    "    print(csv)\n",
    "    df=pd.read_csv(csv)\n",
    "    df=df[[c for c in df.columns if 'Unnamed' not in c]]\n",
    "    ww=pd.DataFrame(df.T[x:].astype(float).sum(axis=1))\n",
    "    ww.columns=['ww']\n",
    "    cn=pd.DataFrame(pd.DataFrame(df[df['Country/Region']=='China']).T[x:].astype(float).sum(axis=1))\n",
    "    cn.columns=['cn']\n",
    "    eu=pd.DataFrame(pd.DataFrame(df[df['Country/Region'].isin(eus)]).T[x:].astype(float).sum(axis=1))\n",
    "    eu.columns=['eu']\n",
    "    df=df[df['Province/State'].astype(str)=='nan']\n",
    "    df=df[df['Country/Region'].isin(countries)]\n",
    "    df=df.set_index(\"Country/Region\").T[x-1:]\n",
    "    df=df[countries]\n",
    "    df.columns=columns\n",
    "    df=df.join(eu).join(cn).join(ww)\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfs[0]-dfs[1]-dfs[2]\n",
    "df=df.drop('ro',axis=1)\n",
    "df=df.stack().reset_index()\n",
    "df.columns=['date','type','value']\n",
    "df=df.set_index('date')\n",
    "df['value']=df['value'].astype(int)\n",
    "df=df.join(szotardf,on='type')\n",
    "df=pd.DataFrame(df.reset_index().set_index(['date','type','value']).stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value','lang','langtype']\n",
    "df.index=pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing global3 from 2021-04-02 00:00:00+00:00 ...\n",
      "Writing to global3 ...\n",
      "81 data points will be written in 0.0162 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['type','lang','langtype']\n",
    "measurement='global3'\n",
    "push2influx(df,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 00:00:00\n"
     ]
    }
   ],
   "source": [
    "n=30\n",
    "start=dfs[0][dfs[0]['ro']>30].index.min()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfs[0].rolling(7).mean().diff()\n",
    "das=[]\n",
    "for c in df:\n",
    "    da=pd.DataFrame(df[c][df[c]>n])\n",
    "    da.index=range(len(da))\n",
    "    das.append(da)\n",
    "das=pd.concat(das,axis=1)\n",
    "das.index=[pd.to_datetime(start)+pd.to_timedelta('1D')*i for i in range(len(das))]\n",
    "df=das.stack().reset_index()\n",
    "df.columns=['date','type','value']\n",
    "df=df.set_index('date')\n",
    "df['value']=df['value'].astype(int)\n",
    "df=df.join(szotardf,on='type')\n",
    "df=pd.DataFrame(df.reset_index().set_index(['date','type','value']).stack()).reset_index().set_index('date')\n",
    "df.columns=['type','value','lang','langtype']\n",
    "df.index=pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing global4 from 2021-05-14 00:00:00+00:00 ...\n",
      "Writing to global4 ...\n",
      "9 data points will be written in 0.0018 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['type','lang','langtype']\n",
    "measurement='global4'\n",
    "push2influx(df,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='governance'\n",
    "df=pd.read_csv(url+sheet)\n",
    "df['date']=pd.to_datetime(df['date'])\n",
    "df=df.set_index('date')[df.columns[10:22]].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "c='gov_note_econ'\n",
    "dc=df[[c,'gov_note_econ_tag','law','law_link']].dropna(subset=[c]).reset_index()\\\n",
    "            .set_index(c).join(szotardf[languages]).reset_index()\n",
    "dc['date']=resolve_time_conflicts(dc['date'])\n",
    "dc=pd.DataFrame(dc.set_index(list(dc.columns[:-3])).stack()).reset_index()\n",
    "dc['index']=dc['law']+dc['level_5']\n",
    "dc=dc.set_index('index')\n",
    "dc.columns=['note','date','tag','law','link','lang','desc']\n",
    "ds=szotardf.stack().reset_index()\n",
    "ds['index']=ds['ID']+ds['level_1']\n",
    "ds=ds.set_index('index')\n",
    "ds.columns=['id','lang2','desc2']\n",
    "dc=dc.join(ds).set_index('date')[['desc','tag','link','lang','desc2']]\n",
    "dc['index']=dc['tag']+dc['lang']\n",
    "ds.columns=['id','lang3','desc3']\n",
    "dc=dc.reset_index().set_index('index').join(ds)\n",
    "dc=dc.set_index('date')[['desc','link','lang','desc2','desc3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing social1 from 2020-04-27 00:00:00+00:00 ...\n",
      "Writing to social1 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['desc','desc2','link','desc3']\n",
    "tag_columns=['lang']\n",
    "measurement='social1'\n",
    "push2influx(dc,measurement,field_columns,tag_columns,shift=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "c='gov_note_social'\n",
    "dc=df[[c,'gov_note_social_tag','law','law_link']].dropna(subset=[c]).reset_index()\\\n",
    "            .set_index(c).join(szotardf[languages]).reset_index()\n",
    "dc['date']=resolve_time_conflicts(dc['date'])\n",
    "dc=pd.DataFrame(dc.set_index(list(dc.columns[:-3])).stack()).reset_index()\n",
    "dc['index']=dc['law']+dc['level_5']\n",
    "dc=dc.set_index('index')\n",
    "dc.columns=['note','date','tag','law','link','lang','desc']\n",
    "ds=szotardf.stack().reset_index()\n",
    "ds['index']=ds['ID']+ds['level_1']\n",
    "ds=ds.set_index('index')\n",
    "ds.columns=['id','lang2','desc2']\n",
    "dc=dc.join(ds).set_index('date')[['desc','tag','link','lang','desc2']]\n",
    "dc['index']=dc['tag']+dc['lang']\n",
    "ds.columns=['id','lang3','desc3']\n",
    "dc=dc.reset_index().set_index('index').join(ds)\n",
    "dc=dc.set_index('date')[['desc','link','lang','desc2','desc3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing social2 from 2020-10-08 00:00:00+00:00 ...\n",
      "Writing to social2 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['desc','desc2','link','desc3']\n",
    "tag_columns=['lang']\n",
    "measurement='social2'\n",
    "push2influx(dc,measurement,field_columns,tag_columns,shift=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "c='gov_note_fin'\n",
    "dc=df[[c,'fin_tag','fin_link','fin_body']].dropna(subset=[c]).reset_index()\\\n",
    "            .set_index(c).join(szotardf[languages]).reset_index()\n",
    "dc['date']=resolve_time_conflicts(dc['date'])\n",
    "dc=pd.DataFrame(dc.set_index(list(dc.columns[:-3])).stack()).reset_index()\n",
    "dc['index']=dc['fin_body']+dc['level_5']\n",
    "dc=dc.set_index('index')\n",
    "dc.columns=['note','date','tag','link','law','lang','desc']\n",
    "ds=szotardf.stack().reset_index()\n",
    "ds['index']=ds['ID']+ds['level_1']\n",
    "ds=ds.set_index('index')\n",
    "ds.columns=['id','lang2','desc2']\n",
    "dc=dc.join(ds).set_index('date')[['desc','tag','link','lang','desc2']]\n",
    "dc['index']=dc['tag']+dc['lang']\n",
    "ds.columns=['id','lang3','desc3']\n",
    "dc=dc.reset_index().set_index('index').join(ds)\n",
    "dc=dc.set_index('date')[['desc','link','lang','desc2','desc3']]\n",
    "dc['desc']=dc['desc'].str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing social3 from 2021-03-16 00:00:00+00:00 ...\n",
      "Writing to social3 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['desc','desc2','link','desc3']\n",
    "tag_columns=['lang']\n",
    "measurement='social3'\n",
    "push2influx(dc,measurement,field_columns,tag_columns,shift=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheet='stocks'\n",
    "# df=pd.read_csv(url+sheet)[:-2].drop('Roman allampapir spreadek',axis=1)\n",
    "# sheet='stocks2008'\n",
    "# df2=pd.read_csv(url+sheet)[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dc=pd.DataFrame(df.set_index(['Dátum']).stack()).reset_index()\n",
    "# dc.columns=['date','stock','value']\n",
    "# dc['year']=2020\n",
    "# dc2=pd.DataFrame(df2.set_index(['Dátum','Dátum 2']).stack()).reset_index()\n",
    "# dc2.columns=['date','date2','stock','value']\n",
    "# dc2['year']=2008\n",
    "# dc3=pd.concat([dc,dc2])\n",
    "# dc3=pd.DataFrame(dc3.set_index('stock').join(szotardf).reset_index().set_index(['index','year','date','date2','value']).stack()).reset_index()\n",
    "# dc3=dc3.set_index('date')\n",
    "# dc3.columns=['stock','year','date2','value','lang','langstock']\n",
    "# dc3['value']=dc3['value'].astype(str).str.replace(',','').str.replace('%','').astype(float)\n",
    "# dc3.index=pd.to_datetime(dc3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_columns=['value']\n",
    "# tag_columns=['lang','langstock','year','date2','stock']\n",
    "# measurement='stocks1'\n",
    "# push2influx(dc3,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='stocks_all'\n",
    "df=pd.read_csv(url+sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks0=['S&P 500','DAX','BET','STOXX 600','FTSE 100']\n",
    "stocksv0=['SP500 volatilitas','DAX volatilitas','BET volatilitas','Stoxx 600 volatilitas','FTSE 100 volatilitas']\n",
    "stocksa0=['WTI','Brent']\n",
    "stocksb0=['ROBOR3M']\n",
    "stocksc0=['Roman allampapir spreadek','Bid/ask 6 honap','Bid/ask 12 honap ','Bid/ask 3 ev','Bid/ask 5 ev','Bid/ask 10 ev']\n",
    "bets0=['BET','BET hozam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[]\n",
    "for s in stocks0:\n",
    "    v.append(process.extractOne(s,list(df.columns))[0])\n",
    "stocks=v\n",
    "v=[]\n",
    "for s in stocksv0:\n",
    "    v.append(process.extractOne(s,list(df.columns))[0])\n",
    "stocksv=v\n",
    "v=[]\n",
    "for s in stocksa0:\n",
    "    v.append(process.extractOne(s,list(df.columns))[0])\n",
    "stocksa=v\n",
    "v=[]\n",
    "for s in stocksb0:\n",
    "    v.append(process.extractOne(s,list(df.columns))[0])\n",
    "stocksb=v\n",
    "v=[]\n",
    "for s in stocksc0:\n",
    "    v.append(process.extractOne(s,list(df.columns))[0])\n",
    "stocksc=v\n",
    "v=[]\n",
    "for s in bets0:\n",
    "    v.append(process.extractOne(s,list(df.columns))[0])\n",
    "bets=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('bets.json','w').write(json.dumps(bets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bets=json.loads(open('bets.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "das=[]\n",
    "dbs=[]\n",
    "for stock in stocks+stocksa:\n",
    "    da=df[[stock,list(df.columns)[list(df.columns).index(stock)+1]]].dropna()\n",
    "    da=da.set_index(stock)\n",
    "    da.columns=[stock]\n",
    "    da.index=pd.to_datetime(da.index)\n",
    "    das.append(da)\n",
    "for i,stock in enumerate(stocks):\n",
    "    da=df[[stock,stocksv[i]]].dropna()\n",
    "    da=da.set_index(stock)\n",
    "    da.columns=[stocksv[i]]\n",
    "    da.index=pd.to_datetime(da.index)\n",
    "    das.append(da)\n",
    "for d in das:\n",
    "    dbs.append(d[~d.index.duplicated(keep='last')])\n",
    "das=pd.concat(dbs,axis=1)\n",
    "for stock in stocksb:\n",
    "    da=df[[stock,list(df.columns)[list(df.columns).index(stock)+1]]].dropna()\n",
    "    da=da.set_index(stock)\n",
    "    da.columns=[stock]\n",
    "    da.index=pd.to_datetime(da.index,dayfirst=True)\n",
    "das=das.join(da)\n",
    "da=df[stocksc].set_index(stocksc[0]).dropna()*100\n",
    "da.index=pd.to_datetime(da.index,dayfirst=True)\n",
    "das=das.join(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "da1=das.sort_index(ascending=False)[:'2020-02-26']\n",
    "da2=das.sort_index(ascending=False)[:'2008-09-16'].sort_index(ascending=False).tail(len(da1))\n",
    "da2.index=da1.index\n",
    "del das #free up mem\n",
    "dc=pd.DataFrame(da1.stack()).reset_index()\n",
    "dc.columns=['date','stock','value']\n",
    "dc['year']=2020\n",
    "dc2=pd.DataFrame(da2.stack()).reset_index()\n",
    "dc2.columns=['date','stock','value']\n",
    "dc2['year']=2008\n",
    "del da1 #free up mem\n",
    "del da2 #free up mem\n",
    "dc3=pd.concat([dc,dc2])\n",
    "del dc\n",
    "del dc2\n",
    "dc3['stock']=dc3['stock'].replace({(stocks+stocksv+stocksa+stocksb+stocksc)[i]:(stocks0+stocksv0+stocksa0+stocksb0+stocksc0)[i] for i in range(len(stocks+stocksv+stocksa+stocksb+stocksc))})\n",
    "dc3=pd.DataFrame(dc3.set_index('stock').join(szotardf).reset_index().set_index(['index','year','date','value']).stack()).reset_index()\n",
    "dc3=dc3.set_index('date')\n",
    "dc3.columns=['stock','year','value','lang','langstock']\n",
    "dc3['value']=dc3['value'].astype(str).str.replace(',','').str.replace('%','').astype(float)\n",
    "dc3.index=pd.to_datetime(dc3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing stocks1 from 2021-04-02 00:00:00+00:00 ...\n",
      "Writing to stocks1 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langstock','year','stock']\n",
    "measurement='stocks1'\n",
    "push2influx(dc3,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='exchangerates'\n",
    "df=pd.read_csv(url+sheet)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc=pd.DataFrame(df.set_index(['date']).stack()).reset_index()\n",
    "dc.columns=['date','stock','value']\n",
    "dc=pd.DataFrame(dc.set_index('stock').join(szotardf)\\\n",
    ".reset_index().set_index(['index','date','value']).stack()).reset_index()\n",
    "dc=dc.set_index('date')\n",
    "dc.columns=['stock','value','lang','langstock']\n",
    "dc['value']=dc['value'].astype(str).str.replace(',','').str.replace('%','').astype(float)\n",
    "dc.index=pd.to_datetime(dc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing stocks2 from 2021-04-02 00:00:00+00:00 ...\n",
      "Writing to stocks2 ...\n",
      "6 data points will be written in 0.0012 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langstock','stock']\n",
    "measurement='stocks2'\n",
    "push2influx(dc,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='firms'\n",
    "df=pd.read_csv(url+sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc=df[['Friendly name','Üzleti forgalom, 2018 (RON)','Alkalmazottak száma, 2018 (fő)',\n",
    "       'Részarány az országos üzleti forgalomból, 2018 (%)','Részarány az iparági üzleti forgalomból, 2018 (%)']]\n",
    "dc.columns=['name','revenue','employees','share','indshare']\n",
    "dc['date']=pd.to_datetime('2020-04-05')\n",
    "dc=dc.set_index('date').dropna(how='all')\n",
    "dc['employees']=dc['employees'].astype(str).str.replace(',','').astype(float).astype(int)\n",
    "dc['revenue']=dc['revenue'].astype(str).str.replace(',','').astype(float).astype(int)\n",
    "dc['share']=dc['share'].astype(str).str.replace('%','').astype(float)\n",
    "dc['indshare']=dc['indshare'].astype(str).str.replace('%','').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_columns=['revenue','employees','share','indshare']\n",
    "tag_columns=['name']\n",
    "measurement='firms1'\n",
    "push2influx(dc,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firms2 at page bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2=df[['Friendly name','Higiénia, távolságtartás', 'Informálás', 'Adomány',\n",
    "       'Üzleti utak leállítása vagy korlátozása',\n",
    "       'Home office, rugalmas munkaidő', 'Részleges leállás', 'Teljes leállás',\n",
    "       'Személyzet csökkentése (akár somai tehnic-el)',\n",
    "       'Támogatás (pl. telefonos vonal stb.)', 'Testhőmérséklet követése',\n",
    "       'Egyéb',\n",
    "       'Súlyosság (4 = nagyon súlyos, 3 = súlyos, 2 = átlagos, 1 = gyenge)']].set_index('Friendly name')\n",
    "dc3=dc.reset_index().set_index('name').join(dc2).reset_index().set_index('date')\n",
    "dc3.columns=['name','revenue','employees','share','indshare','higenia',\n",
    "'informalas',\n",
    "'adomany',\n",
    "'uzleti_utak',\n",
    "'home_office',\n",
    "'reszleges_leallas',\n",
    "'teljes_leallas',\n",
    "'szemelyzet_csok',\n",
    "'tamogatas',\n",
    "'testho',\n",
    "'egyebi',\n",
    "'sulyos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_columns=list(dc3.columns[1:])\n",
    "tag_columns=['name']\n",
    "measurement='firms3'\n",
    "push2influx(dc3.fillna(0),measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2=df[['Friendly name','COVID-19 terjedésének megakadályozása','Alkalmazottak egészségének védelme','Ügyfelek egészségének védelme','Csökkent kereslet','Támogatás (egészségügyi, kapcsolattartás)',\n",
    "        'A vállalat lépéseinek indoka(i) HU','A vállalat lépéseinek indoka(i) RO','A vállalat lépéseinek indoka(i) EN',\n",
    "       'Súlyosság (4 = nagyon súlyos, 3 = súlyos, 2 = átlagos, 1 = gyenge)']].set_index('Friendly name')\n",
    "dc3=dc.reset_index().set_index('name').join(dc2).reset_index().set_index('date')\n",
    "dc3.columns=['name','revenue','employees','share','indshare','terjedes',\n",
    "'alkegeszseg',\n",
    "'ugyfegeszseg',\n",
    "'csokkereslet',\n",
    "'tamogatasek',\n",
    "'HU','RO','EN',\n",
    "'sulyos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc3=dc3.reset_index().set_index(['date','name','revenue','employees','share','indshare','terjedes',\n",
    "'alkegeszseg',\n",
    "'ugyfegeszseg',\n",
    "'csokkereslet',\n",
    "'tamogatasek',\n",
    "'sulyos']).stack().reset_index().set_index('date')\n",
    "dc3.columns=['name','revenue','employees','share','indshare','terjedes',\n",
    "'alkegeszseg',\n",
    "'ugyfegeszseg',\n",
    "'csokkereslet',\n",
    "'tamogatasek',\n",
    "'sulyos',\n",
    "'lang',\n",
    "'lepesindok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_columns=['revenue','employees','share','indshare','terjedes',\n",
    "'alkegeszseg',\n",
    "'ugyfegeszseg',\n",
    "'csokkereslet',\n",
    "'tamogatasek',\n",
    "'sulyos',\n",
    "'lepesindok']\n",
    "tag_columns=['name','lang']\n",
    "measurement='firms4'\n",
    "push2influx(dc3.fillna(0),measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='sajto'\n",
    "df=pd.read_csv(url+sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index('Datum')\n",
    "df.index=pd.to_datetime(df.index)\n",
    "df.columns=['type','media','desc','lang','link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure datum is unique\n",
    "ds={}\n",
    "ts=[]\n",
    "for i in df.T.iteritems():\n",
    "    d=str(i[0])\n",
    "    if d not in ds:\n",
    "        ds[d]=pd.to_datetime(d)\n",
    "        t=(ds[d])\n",
    "    else:\n",
    "        ds[d]=ds[d]+pd.to_timedelta('1m')\n",
    "        t=(ds[d])\n",
    "    ts.append(t)\n",
    "df['Datum1']=ts\n",
    "df=df.set_index('Datum1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_icons={\n",
    "    'video':'🎥', \n",
    "    'news':'📰', \n",
    "    'talk':'🎤'\n",
    "}\n",
    "df['type']=df['type'].replace(news_icons)\n",
    "df['desc']=df['desc'].str.replace('\\\"','\\`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing news1 from 2021-09-04 00:00:00+00:00 ...\n",
      "Writing to news1 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['type','desc','lang','link','media']\n",
    "tag_columns=[]\n",
    "measurement='news1'\n",
    "push2influx(df,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso3_2={'BRA':'br','FRA':'fr','DEU':'de','GBR':'gb','ITA':'it','ROU':'ro','USA':'us'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deprecated 2021-02-02\n",
    "# stringency=requests.get('https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-02-26/'+now).content\n",
    "# stringency=json.loads(stringency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringdata=[]\n",
    "# for date in stringecy['data']:\n",
    "#     for country in stringecy['data'][date]:\n",
    "#         if country in iso3_2:\n",
    "#             stringdata.append({'date':date,'iso3':country,\n",
    "#                                'stringency':stringecy['data'][date][country]['stringency'],'cases':stringecy['data'][date][country]['confirmed']})\n",
    "# df=pd.DataFrame(stringdata)\n",
    "# df=df.set_index(['date','iso3']).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "stringency_csv=pd.read_csv('https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv')\n",
    "stringency_csv=stringency_csv[((stringency_csv['CountryCode'].isin(iso3_2))&\\\n",
    "                               (stringency_csv['Jurisdiction']=='NAT_TOTAL'))]\n",
    "stringency_csv.index=pd.to_datetime(stringency_csv['Date'].astype(str).str[:4]+'-'+\\\n",
    "    stringency_csv['Date'].astype(str).str[4:6]+'-'+\\\n",
    "    stringency_csv['Date'].astype(str).str[6:])\n",
    "stringency_csv=stringency_csv[['CountryCode','StringencyIndexForDisplay','ConfirmedCases']].loc['2020-02-26':]\n",
    "stringency_csv.columns=['country','stringency','cases']\n",
    "df=stringency_csv.set_index('country',append=True).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['date','iso3','type','value']\n",
    "df=df.join(pd.DataFrame(iso3_2,index=['country']).T,on='iso3').join(szotardf[languages],on='country')\\\n",
    "    .drop(['iso3'],axis=1).set_index(['date','value','type','country']).stack().reset_index()\\\n",
    "    .join(szotardf[languages],on='type')\n",
    "df.columns=['date','value','type','lang','country','langcountry']+languages\n",
    "df=df.set_index(['date','value','type','lang','country','langcountry']).stack().reset_index()\n",
    "df.columns=['date','value','type','country','lang','langcountry','lang2','langtype']\n",
    "df=df.set_index('date')\n",
    "df.index=pd.to_datetime(df.index)\n",
    "df['value']=df['value'].astype(float)\n",
    "severity=df[df['country']=='ro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity.to_csv('severity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# severity=pd.read_csv('severity.csv').set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing severity from 2021-04-03 00:00:00+00:00 ...\n",
      "Writing to severity ...\n",
      "315 data points will be written in 0.063 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','country','lang2','type','langcountry','langtype']\n",
    "measurement='severity'\n",
    "push2influx(df,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mobility: 125.5 MiB\n",
      "                      datagov4: 56.2 MiB\n",
      "                            df: 19.4 MiB\n",
      "                           dc3: 18.4 MiB\n",
      "                      severity:  2.8 MiB\n",
      "                      datagov3:  1.8 MiB\n",
      "                            dp:  1.6 MiB\n",
      "                           df4:  1.3 MiB\n",
      "                         locs2:  1.2 MiB\n",
      "                          locs: 986.5 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "def memo():\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                             key= lambda x: -x[1])[:10]:\n",
    "        print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _60\n",
    "del _66\n",
    "del _67\n",
    "del _69\n",
    "del df\n",
    "del mobility\n",
    "del datagov4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google mobility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobility counties - restart `InfluxDB` before proceeding with this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='industry_county'\n",
    "dc=pd.read_csv(url+sheet)[:-2]\n",
    "dc=dc[dc.columns[:3]].dropna()\n",
    "dc.columns=['HU','RO','EN']\n",
    "dc.loc['ro']=szotardf.loc['ro'][['HU','RO','EN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement='mobility'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try migratin to dask\n",
    "google_mobility_url='https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv'\n",
    "field_columns=['retail_and_recreation_percent_change_from_baseline',\n",
    "       'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "       'parks_percent_change_from_baseline',\n",
    "       'transit_stations_percent_change_from_baseline',\n",
    "       'workplaces_percent_change_from_baseline',\n",
    "       'residential_percent_change_from_baseline']\n",
    "tag_columns=['sub_region_1']\n",
    "filter_columns=['country_region_code']\n",
    "time_columns=['date']\n",
    "df_type={i:'Int64' for i in field_columns}\n",
    "df_type={i:str for i in time_columns}#'M8[s]'\n",
    "df_type.update({i:str for i in tag_columns+filter_columns})\n",
    "iter_csv = pd.read_csv(google_mobility_url, usecols=field_columns+tag_columns+filter_columns+time_columns, \n",
    "                       dtype=df_type, iterator=True, chunksize=1000)\n",
    "# latest=client.query('SELECT * FROM '+measurement+' GROUP BY * ORDER BY DESC LIMIT 1')\n",
    "# lat=latest[list(latest.keys())[0]].index[0]\n",
    "# df = pd.concat([chunk[((chunk['country_region_code'] == 'RO')&(pd.to_datetime(chunk['date'])>lat.replace(tzinfo=None)))] for chunk in iter_csv])\n",
    "df = pd.concat([chunk[(chunk['country_region_code'] == 'RO')] for chunk in iter_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(subset=field_columns) #temporary workaround to remove all rows with at least one NaN, in order to counteract Google changing the reporting style #https://www.google.com/covid19/mobility/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index('date')\n",
    "df.index=pd.to_datetime(df.index)\n",
    "df['sub_region_1']=df['sub_region_1'].str.replace(' County','')\n",
    "field_columns=['retail_and_recreation_percent_change_from_baseline',\n",
    "       'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "       'parks_percent_change_from_baseline',\n",
    "       'transit_stations_percent_change_from_baseline',\n",
    "       'workplaces_percent_change_from_baseline',\n",
    "       'residential_percent_change_from_baseline']\n",
    "tag_columns=['sub_region_1']\n",
    "df=df[field_columns+tag_columns]\n",
    "df['sub_region_1']=df['sub_region_1'].fillna(dc.loc['ro']['RO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RESCALED BY FEB 15 - MAR 1\n",
    "df=df.reset_index()\n",
    "dki=df.set_index(['date','sub_region_1'])\n",
    "# dkm=df.set_index('date').[:'2020-03-01'].groupby(['sub_region_1']).mean()+100\n",
    "# dkm=(df.groupby(['date','sub_region_1']).mean()+100).loc[:'2020-03-01']\n",
    "dkm=df.set_index('date').loc[:'2020-03-01'].groupby(['sub_region_1']).mean()+100\n",
    "df=(dki/dkm).reset_index().set_index('date')\n",
    "\n",
    "df=pd.DataFrame(df.set_index('sub_region_1',append=True).stack()).reset_index()\n",
    "df['sub_region_1']=df['sub_region_1'].replace('Bucharest','București')\n",
    "df=df.set_index('sub_region_1').join(dc.set_index('RO',drop=False))\n",
    "df.columns=['date','indicator','value','HU','RO','EN']\n",
    "mobility=pd.DataFrame(df.set_index(['date','indicator','value']).stack()).reset_index().set_index('date')\n",
    "mobility.columns=['indicator','value','lang','langtype']\n",
    "mobility['value']=np.round(mobility['value']*100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility.to_csv('mobility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobility=pd.read_csv('mobility.csv').set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing mobility from 2021-03-29 00:00:00+00:00 ...\n",
      "Writing to mobility ...\n",
      "4248 data points will be written in 0.8496 batches.\n",
      "Expected query running time is: 4 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['indicator','lang','langtype']\n",
    "push2influx(mobility,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobility dimensions - restart `InfluxDB` before proceeding with this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=pd.DataFrame(szotardf[['HU','RO','EN']].stack())\n",
    "ds.index.names=['indicator','lang']\n",
    "df=mobility.reset_index().set_index(['indicator','lang']).join(ds).reset_index().set_index('date')\n",
    "df.columns=['indicator','lang','value','langtype','langindicator']\n",
    "df=df[df['langtype'].isin(szotardf.loc['ro'][['HU','RO','EN']].values)][['value','lang','langindicator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing mobility2 from 2021-03-29 00:00:00+00:00 ...\n",
      "Writing to mobility2 ...\n",
      "54 data points will be written in 0.0108 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langindicator']\n",
    "measurement='mobility2'\n",
    "push2influx(df,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Border crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='bordercrossings'\n",
    "db=pd.read_csv(url+sheet)\n",
    "db['date']=pd.to_datetime(db['Dátum'])\n",
    "db=db.set_index('date')[['Változás ']]\n",
    "db.columns=['border']\n",
    "db['border']=db['border'].astype(str).str.replace(',','').str.replace('%','').astype(float).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Border crossings - automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol='https://www.politiadefrontiera.ro/ro/main/n-date-deschise-17/?s=compact&page_no='\n",
    "sources={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for page in range(0,2):\n",
    "    print(page)\n",
    "    urlp=pol+str(page)\n",
    "    r = requests.get(urlp)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    s=soup.find('div',{'class':'news_grid_home'}).find_all('div',{'class':'article-compact'})\n",
    "    dates=[i.find('time')['datetime'] for i in s]\n",
    "    links=[i.find('a')['href'] for i in s]\n",
    "    for i,link in enumerate(links):\n",
    "        date=dates[i]\n",
    "        if date not in sources:\n",
    "            r = requests.get(link)\n",
    "            html_content = r.text\n",
    "            soup = BeautifulSoup(html_content, 'lxml')\n",
    "            sources[date]=soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in sources:\n",
    "    open(htmlipath+'panels/daily/sources'+date+'.txt','w').write(json.dumps(str(sources[date])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2021-04-05', '2021-03-13')"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sources.keys())[0],list(sources.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath=htmlipath+'panels/daily/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "sources={i[7:17]:json.loads(open(mypath+i,'r' ).read()) for i in onlyfiles if 'sources' in i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tir 1 2021-01-09 0 https://www.politiadefrontiera.ro/ro/main/i-situatia-traficului-la-granita-de-vest-in-ultimele-24-de-ore-22224.html\n",
      "tir 1 2021-01-03 4300 https://www.politiadefrontiera.ro/ro/main/i-situatia-traficului-la-frontiera-de-vest-in-ultimele-24-de-ore-22113.html\n",
      "vehicles 1 2021-01-02 6100 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22112.html\n",
      "tir 1 2021-01-02 2700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22112.html\n",
      "enter 1 2021-01-02 6300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22112.html\n",
      "exit 0 2021-01-02 7100 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22112.html\n",
      "tir 1 2020-12-31 4700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22099.html\n",
      "tir 1 2020-12-29 4800 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22062.html\n",
      "tir 1 2020-12-28 4300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22052.html\n",
      "vehicles 1 2020-12-27 9600 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22048.html\n",
      "tir 1 2020-12-27 3300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22048.html\n",
      "tir 1 2020-12-26 3400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-22046.html\n",
      "enter 1 2020-05-05 9000 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20129.html\n",
      "enter 1 2020-05-04 8400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20121.html\n",
      "enter 1 2020-04-28 9300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20097.html\n",
      "enter 1 2020-04-27 8400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20090.html\n",
      "exit 0 2020-04-27 8900 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20090.html\n",
      "enter 1 2020-04-24 9900 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20078.html\n",
      "enter 1 2020-04-23 9900 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20076.html\n",
      "enter 1 2020-04-22 8400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20073.html\n",
      "exit 0 2020-04-22 9200 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20073.html\n",
      "vehicles 1 2020-04-21 8900 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20069.html\n",
      "enter 1 2020-04-21 6400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20069.html\n",
      "exit 0 2020-04-21 7300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20069.html\n",
      "exit 0 2020-04-18 8900 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20067.html\n",
      "enter 1 2020-04-16 9400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore----20061.html\n",
      "exit 0 2020-04-16 9600 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore----20061.html\n",
      "enter 1 2020-04-15 7700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20058.html\n",
      "exit 0 2020-04-15 9700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20058.html\n",
      "enter 1 2020-04-14 6400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20025.html\n",
      "exit 0 2020-04-14 8700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20025.html\n",
      "vehicles 1 2020-04-13 9800 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20019.html\n",
      "enter 1 2020-04-13 7300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20019.html\n",
      "exit 0 2020-04-13 6600 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20019.html\n",
      "enter 1 2020-04-12 9300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20018.html\n",
      "exit 0 2020-04-12 8500 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20018.html\n",
      "exit 0 2020-04-11 9000 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20017.html\n",
      "exit 0 2020-04-09 9200 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20006.html\n",
      "exit 0 2020-04-08 9400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20004.html\n",
      "enter 1 2020-04-07 9200 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20001.html\n",
      "exit 0 2020-04-07 9100 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-20001.html\n",
      "exit 0 2020-04-06 6300 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19999.html\n",
      "exit 0 2020-04-05 8400 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19996.html\n",
      "exit 1 2020-04-04 9600 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19991.html\n",
      "exit 1 2020-04-02 9200 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19985.html\n",
      "exit 1 2020-04-01 9700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19977.html\n",
      "exit 1 2020-03-31 9600 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19974.html\n",
      "exit 1 2020-03-30 7700 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19969.html\n",
      "exit 1 2020-03-29 8500 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19968.html\n",
      "exit 1 2020-03-28 8600 https://www.politiadefrontiera.ro/ro/main/i-situatia-la-frontiera-in-ultimele-24-de-ore-19967.html\n"
     ]
    }
   ],
   "source": [
    "for date in sources:\n",
    "    soup=BeautifulSoup(sources[date], 'lxml')\n",
    "    if 'sfârșit de săptămână' in soup.find('title').text:\n",
    "        stop=True\n",
    "    elif 'ultimele 48 de ore' in soup.find('title').text:\n",
    "        stop=True\n",
    "    elif '24-25 decembrie' in soup.find('title').text:\n",
    "        stop=True\n",
    "    elif 'ultima săptămână' in soup.find('title').text:\n",
    "        stop=True\n",
    "    elif 'ultima săptămână' in soup.find('meta')['content']:\n",
    "        stop=True\n",
    "    elif 'ultimele 48 de ore' in soup.find('meta')['content']:\n",
    "        stop=True\n",
    "    elif 'sfârșit de săptămână' in soup.find('meta')['content']:\n",
    "        stop=True\n",
    "    else:\n",
    "        stop=False\n",
    "    if pd.to_datetime(date)>pd.to_datetime('2018-08-09'):\n",
    "        if date=='2020-01-27': stop=True\n",
    "        elif date=='2020-01-02': stop=True\n",
    "        elif date=='2019-12-29': stop=True\n",
    "        if not stop:\n",
    "            \n",
    "            if date not in frontier:\n",
    "                try:\n",
    "                    for j in range(7):\n",
    "                        art=soup.find('div',{'class':'txtcontent'}).findAll('p')[j].text.replace(u'\\xa0',' ')\n",
    "                        art=art.replace(' peste ',' ')\n",
    "                        if 'sensul de intrare' in art:\n",
    "                            break\n",
    "                except:\n",
    "                    print('no j',date,soup.findAll('link')[1]['href'])\n",
    "                    break\n",
    "                date2=art[art.find('data')+4:art.find('data')+15].strip()\n",
    "                vehicles=art[art.find('mijloace de transport')-8:art.find('mijloace de transport')].strip()\n",
    "                \n",
    "                #vehicles\n",
    "                try:\n",
    "                    vehicles=int(vehicles.replace('.','').replace('e ','')\\\n",
    "                                .replace('i ','').replace('u ',''))\n",
    "                    if vehicles<10000:\n",
    "                        print('vehicles 1',date,vehicles,soup.findAll('link')[1]['href'])\n",
    "                except:\n",
    "                    if date=='2020-05-29': vehicles=23600\n",
    "                    elif date=='2020-05-03': vehicles=25900\n",
    "                    elif date=='2020-04-20': vehicles=16700\n",
    "                    elif date=='2020-04-18': vehicles=12600\n",
    "                    elif date=='2019-08-25': vehicles=105100\n",
    "                    elif date=='2018-08-09': vehicles=53000\n",
    "                    elif date=='2018-08-08': vehicles=53040\n",
    "                    else: \n",
    "                        print('vehicles 2',j, date,vehicles,soup.findAll('link')[1]['href'])\n",
    "                        break\n",
    "                \n",
    "                #tir\n",
    "                if date=='2021-03-21':\n",
    "                    tir=11650\n",
    "                elif pd.to_datetime(date)>pd.to_datetime('2020-05-15'):\n",
    "                    tir=art[art.find('automarfare')-9:art.find('automarfare')].strip()\n",
    "                    try:\n",
    "                        tir=int(tir.replace('.','').replace('re ','')\\\n",
    "                                .replace('e ','').replace('v ','').replace(' de',''))\n",
    "                        if tir<5000:\n",
    "                            print('tir 1',date,tir,soup.findAll('link')[1]['href'])\n",
    "                    except:\n",
    "                        if date=='2021-01-09': tir=10000\n",
    "                        else: \n",
    "                            print('tir 2',j, date,tir,soup.findAll('link')[1]['href'])\n",
    "                            break\n",
    "                else:\n",
    "                    tir=np.nan\n",
    "                \n",
    "                #enter\n",
    "                #enter=art[art.find('aproximativ ')+11:art.find('aproximativ ')+19].strip()\n",
    "                enter=art[art.find('sensul de intrare au fost aproximativ ')+37:\\\n",
    "                        art.find('sensul de intrare au fost aproximativ ')+46].strip()\n",
    "                try:\n",
    "                    enter=int(enter.replace('.','').replace('re ','')\\\n",
    "                            .replace('e ','').replace('v ','').replace(' de','')\\\n",
    "                            .replace(' pe','').replace(' p',''))\n",
    "                    if enter==0:\n",
    "                        if date=='2021-01-09': enter=31700\n",
    "                    elif enter==24:\n",
    "                        enter=art[art.lower().find(' pe sensul de intrare ')-8:\\\n",
    "                                  art.lower().find(' pe sensul de intrare ')].strip()\n",
    "                        enter=int(enter.replace('.','').replace('re ','')\\\n",
    "                            .replace('e ','').replace('v ','').replace(' de','')\\\n",
    "                            .replace(' pe','').replace(' p',''))\n",
    "                    elif enter<10000:\n",
    "                        print('enter 1',date,enter,soup.findAll('link')[1]['href'])\n",
    "                except:\n",
    "                    if date=='2019-08-25': enter=174700\n",
    "                    elif date=='2019-02-06': enter=55100\n",
    "                    elif date=='2019-12-22': enter=209000\n",
    "                    elif date=='2019-12-21': enter=134500\n",
    "                    elif date=='2018-12-23': enter=176500\n",
    "                    elif date=='2018-12-22': enter=122200\n",
    "                    elif date=='2018-04-02': enter=np.nan\n",
    "                    elif date=='2018-12-14': enter=69300\n",
    "                    elif date=='2018-10-16': enter=77600\n",
    "                    else: \n",
    "                        enter=art[art.lower().find(' pe sensul de intrare ')-8:\\\n",
    "                                  art.lower().find(' pe sensul de intrare ')].strip()\n",
    "                        try:\n",
    "                            enter=int(enter.replace('.','').replace('re ','')\\\n",
    "                                .replace('e ','').replace('v ','').replace(' de','')\\\n",
    "                                .replace(' pe','').replace(' p',''))\n",
    "                        except:\n",
    "                            enter=art[art.find('sensul de intrare au fost ')+25:\\\n",
    "                                       art.find('sensul de intrare au fost ')+34].strip()\n",
    "                            try:\n",
    "                                enter=int(enter.replace('.','').replace('re ','')\\\n",
    "                                    .replace('e ','').replace('v ','').replace(' de','')\\\n",
    "                                    .replace(' pe','').replace(' p',''))\n",
    "                            except:                                \n",
    "                                print('enter 2',j, date,enter,soup.findAll('link')[1]['href'])\n",
    "                                break\n",
    "                \n",
    "                #exit\n",
    "                exit=art[art.find('ieşire')+6:art.find('ieşire')+14].strip()\n",
    "                try:\n",
    "                    exit=int(exit.replace('.','').replace('re ','')\\\n",
    "                            .replace('e ','').replace('v ','').replace(' de','')\\\n",
    "                            .replace(' p',''))\n",
    "                    if exit==0:\n",
    "                        if date=='2021-01-09': enter=44300\n",
    "                    elif exit<10000:\n",
    "                        print('exit 0',date,exit,soup.findAll('link')[1]['href'])\n",
    "                except:\n",
    "                    exit=art[art.lower().find('sensul de intrare şi')+20:\\\n",
    "                             art.lower().find('sensul de intrare şi')+28].strip()\n",
    "                    try:\n",
    "                        exit=int(exit.replace('.','').replace('re ','')\\\n",
    "                            .replace('e ','').replace('v ','').replace(' de','')\\\n",
    "                            .replace(' p',''))\n",
    "                        if exit<10000:\n",
    "                            print('exit 1', date,exit,soup.findAll('link')[1]['href'])\n",
    "                    except:\n",
    "                        if date=='2023-08-25': exit=174700\n",
    "                        elif date=='2020-02-09': exit=156600\n",
    "                        elif date=='2020-01-19': exit=186400\n",
    "                        elif date=='2019-12-26': exit=np.nan\n",
    "                        elif date=='2019-12-22': exit=105000\n",
    "                        elif date=='2019-12-21': exit=90500\n",
    "                        elif date=='2019-12-15': exit=158300\n",
    "                        elif date=='2019-11-17': exit=174300\n",
    "                        elif date=='2019-10-20': exit=610800\n",
    "                        elif date=='2019-09-22': exit=219400\n",
    "                        elif date=='2019-08-25': exit=np.nan\n",
    "                        elif date=='2018-04-02': exit=np.nan\n",
    "                        elif date=='2018-12-24': exit=83400\n",
    "                        elif date=='2018-12-13': exit=59200\n",
    "                        else: \n",
    "                            print('exit 2',j, date,exit,soup.findAll('link')[1]['href'],art)\n",
    "                            break\n",
    "                \n",
    "                frontier[date]={'date':date,'date2':date2,\n",
    "                                'url':soup.findAll('link')[1]['href'],\n",
    "                                'vehicles':vehicles,'tir':tir,'enter':enter,'exit':exit}\n",
    "        else:\n",
    "            frontier[date]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2=pd.DataFrame(frontier).T\n",
    "db2.index=pd.to_datetime(db2.index)\n",
    "db2=db2.sort_index()\n",
    "db2=db2.resample('1d').sum().replace(0,np.nan)#.interpolate(method='linear')\n",
    "db2['net entry']=(db2['enter']-db2['exit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_change={}\n",
    "for i in db2.loc['2020-01-01':].T.iteritems():\n",
    "    try:\n",
    "        # ly=pd.to_datetime(str(int(str(i[0])[:4])-1)+str(i[0])[4:]) #tavalyi ev\n",
    "        ly=pd.to_datetime('2019'+str(i[0])[4:]) #2019\n",
    "        tm20=i[1]['enter']+i[1]['exit']\n",
    "        tm19=db2.loc[ly]['enter']+db2.loc[ly]['exit']\n",
    "        # tm20=i[1]['vehicles']\n",
    "        # tm19=db2.loc[ly]['vehicles']\n",
    "        change=(tm20-tm19)*100/tm19\n",
    "    except:\n",
    "        change=np.nan\n",
    "    border_change[i[0]]=change\n",
    "border_change=pd.DataFrame(border_change,index=['border_change']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2=db2.join(border_change.join(db).bfill(axis=1)['border_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3=db2[['vehicles','tir','enter','exit','border_change','net entry']].stack().reset_index()\\\n",
    "    .set_index('level_0').dropna().join(szotardf[languages],on='level_1')\n",
    "db3=pd.DataFrame(db3.set_index(['level_1',0],append=True).stack())\n",
    "db3.columns=['a']\n",
    "db3=db3.reset_index().set_index('level_0')\n",
    "db3.columns=['type','value','lang','langtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing mobility3 from 2021-04-03 00:00:00+00:00 ...\n",
      "Writing to mobility3 ...\n",
      "54 data points will be written in 0.0108 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langtype','type']\n",
    "measurement='mobility3'\n",
    "# push2influx(db3.loc['2020-02-26':],measurement,field_columns,tag_columns,fo=True)\n",
    "push2influx(db3.loc['2020-02-26':],measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobility4 at page bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobility index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mobility: 126.5 MiB\n",
      "                      datagov4: 56.2 MiB\n",
      "                           dc3: 18.4 MiB\n",
      "                           db3:  2.9 MiB\n",
      "                      severity:  2.8 MiB\n",
      "                      datagov3:  1.8 MiB\n",
      "                            dp:  1.6 MiB\n",
      "                           dki:  1.5 MiB\n",
      "                           df4:  1.3 MiB\n",
      "                         locs2:  1.2 MiB\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "\n",
    "# # These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=mobility.reset_index().set_index(['indicator','lang']).join(ds).reset_index().set_index('date')\\\n",
    "    .set_index(['indicator','lang']).loc['residential_percent_change_from_baseline'].loc['HU'][0].values[0]\n",
    "dz=df[df['lang']=='HU'].set_index('langindicator',append=True)[['value']].unstack()['value']\n",
    "dz[home]=-dz[home]\n",
    "dz=pd.DataFrame(dz.T.mean())\n",
    "dz.columns=['mobility_mini']\n",
    "mobility_mini=dz.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_mini.to_csv('mobility_mini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobility_mini=pd.read_csv('mobility_mini.csv').set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Company registry - industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='matrix'\n",
    "df=pd.read_csv(url+sheet)[:-1]\n",
    "df=df[['IND HU short','IND RO short','IND EN short','Industry CAEN']].set_index('Industry CAEN')\n",
    "df.columns=languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='CegekCAEN'\n",
    "db=pd.read_csv(url+sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=db.set_index('CAEN kód').join(df)\n",
    "dd=pd.DataFrame(dd.set_index(['Hónap', 'Típus','Aktív cégek száma összesen (2018)', 'Cégek száma (2020)',\n",
    "       'Cégek száma (2019)', 'Változás az előző évhez képest (%)',\n",
    "       'Aktív cégek számához arányosítva (%)']).stack()).reset_index().reset_index()\n",
    "dd.columns=['index','Hónap', 'Típus','active2018', 'comp2020','comp2019', 'changeY','changeA','lang','sector']\n",
    "ds=pd.DataFrame(szotardf[languages].stack())\n",
    "ds.index.names=['Típus','lang']\n",
    "ds.columns=['type']\n",
    "dd=dd.set_index(['Típus','lang']).join(ds).reset_index()\n",
    "for c in [ 'changeY','changeA']:\n",
    "    dd[c]=(dd[c].astype(str).str.replace(',','').str.replace('%','').astype(float)/100.0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.index=pd.to_datetime('2020-'+(pd.to_datetime('2020-'+dd['Hónap'].astype(str)).dt.month+1).astype(str)+'-1')-pd.to_timedelta('1d')\n",
    "dd.index.name='date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing registry1 from 2020-08-01 00:00:00+00:00 ...\n",
      "Writing to registry1 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['active2018', 'comp2020','comp2019', 'changeY','changeA']\n",
    "tag_columns=['lang','sector','type']\n",
    "dd=dd[field_columns+tag_columns]\n",
    "measurement='registry1'\n",
    "push2influx(dd,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23776"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save for Flourish\n",
    "de=dd.reset_index().set_index(['lang','date','sector','type']).loc['RO']['changeY'].unstack().reset_index()\n",
    "de['date']=de['date'].astype(str)\n",
    "open(htmlipath+'panels/registry1.json','w').write(json.dumps(list(de.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24189"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save for Flourish\n",
    "de=dd.reset_index().set_index(['lang','date','type','sector']).loc['RO']['changeY'].unstack().reset_index()\n",
    "de['date']=de['date'].astype(str)\n",
    "open(htmlipath+'panels/registry1b.json','w').write(json.dumps(list(de.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18070"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save for Flourish\n",
    "de=dd.reset_index().set_index(['lang','sector','type','date']).loc['RO']['changeY'].reset_index()\n",
    "de['date']=de['date'].astype(str)\n",
    "de=de.set_index(['sector','type','date'])['changeY'].unstack().reset_index()\n",
    "open(htmlipath+'panels/registry1c.json','w').write(json.dumps(list(de.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobility_mini=mobility_mini[:'2020-08-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='minidashboard'\n",
    "dm=pd.read_csv(url+sheet)\n",
    "# sheet='governance'\n",
    "# df=pd.read_csv(url+sheet)\n",
    "# df['date']=pd.to_datetime(df['date'])\n",
    "# df=df[df.columns[:10]].set_index('date')\n",
    "# df=df0[list(dl.columns)+['tests','test']].dropna(how='all')\n",
    "df=df0[list(dl.columns)].dropna(how='all')\n",
    "df=df.loc[~df.index.isnull()]\n",
    "for i in dm.set_index('ID')['UI'].iteritems():\n",
    "    df[i[0]]=i[1]\n",
    "sheet='EcMonitor'\n",
    "du=pd.read_csv(url+sheet)\n",
    "du=du.set_index('date').loc['Unemployment ratio'][2:].dropna()\n",
    "du.index=[(pd.to_datetime('now')-pd.to_timedelta('11D')*(1+i)).strftime('%Y-%m-%d') for i in range(len(du))][::-1]\n",
    "du=pd.DataFrame(du)\n",
    "du.columns=['unemployment']\n",
    "du.index=pd.to_datetime(du.index)\n",
    "df=df.join(du)\n",
    "sheet='employmentdata'\n",
    "de=pd.read_csv(url+sheet)\n",
    "de['date']=pd.to_datetime(de['date'])\n",
    "de=de[de.columns[:3]].set_index('date')\n",
    "de.columns=['felbontott','felfuggesztett']\n",
    "# df=df.join(de.ffill())\n",
    "for c in de.columns:\n",
    "    de[c]=de[c].astype(str).str.replace(',','').str.replace('%','').astype(float)#.interpolate(method='linear').dropna()\n",
    "df=df.join(de)\n",
    "df=df.join(mobility_mini)\n",
    "for c in df.columns:\n",
    "    df[c]=df[c].astype(str).str.replace(',','').str.replace('%','').astype(float)\n",
    "sheet='exchangerates'\n",
    "dx=pd.read_csv(url+sheet)\n",
    "dx=dx[['date','EUR - RON (megváltozás)']][5:-1]\n",
    "dx['date']=pd.to_datetime(dx['date'])\n",
    "dx=dx.set_index('date')\n",
    "dx.columns=['xch']\n",
    "dx['xch']=dx['xch'].astype(str).str.replace(',','').str.replace('%','').astype(float).sort_index().cumsum()\n",
    "df=df.join(dx)\n",
    "df['xch']=df['xch'].ffill()\n",
    "sheet='stocks_all'\n",
    "dk=pd.read_csv(url+sheet)[bets].dropna()\n",
    "dk['date']=pd.to_datetime(dk[bets[0]])\n",
    "dk=dk.set_index('date').drop(bets[0],axis=1)\n",
    "dk.columns=['bet']\n",
    "dk=dk.reindex(df.index)\n",
    "dk['bet']=dk['bet'].astype(str).str.replace(',','').str.replace('%','').astype(float).sort_index().cumsum()\n",
    "df=df.join(dk)\n",
    "sheet='MiniGDP'\n",
    "dg=pd.read_csv(url+sheet)\n",
    "dg1=dg[['Date','GDP (QoQ, SA)']].dropna()\n",
    "dg1.index=[(pd.to_datetime('now')-pd.to_timedelta('2D')*(1+i)).strftime('%Y-%m-%d') for i in range(len(dg1))][::-1]\n",
    "dg2=dg[['Date.1','GDP']].dropna()\n",
    "dg2.index=[(pd.to_datetime('now')-pd.to_timedelta('8D')*(1+i)).strftime('%Y-%m-%d') for i in range(len(dg2))][::-1]\n",
    "sheet='DFMChartYoY'\n",
    "dg3=pd.read_csv(url+sheet).T\n",
    "dg3=pd.read_csv(url+sheet,skiprows=19,nrows=2).T.iloc[1:].dropna()[1]\n",
    "dg3.index=[(pd.to_datetime('now')-pd.to_timedelta('1D')*(1+i)).strftime('%Y-%m-%d') for i in range(len(dg3))][::-1]\n",
    "dg=dg1.join(dg2).join(dg3)[['GDP','GDP (QoQ, SA)',1]].astype(float)\n",
    "dg.columns=['gdp','gdpq','employees']\n",
    "dg.index=pd.to_datetime(dg.index)\n",
    "df=df.join(dg)\n",
    "df=df[:pd.to_datetime('now')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing governance3 from 2021-04-06 00:00:00+00:00 ...\n",
      "Writing to governance3 ...\n",
      "0 data points will be written in 0.0 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=list(df.columns)\n",
    "tag_columns=[]\n",
    "measurement='governance3'\n",
    "# push2influx(df,measurement,field_columns,tag_columns,wo=True,fo=True)\n",
    "push2influx(df,measurement,field_columns,tag_columns,wo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('405', 76210, '06', '23:07')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today0=now[-2:]\n",
    "if today0[0]=='0': \n",
    "    today=today0[1]\n",
    "else:\n",
    "    today=today0\n",
    "currents=str(current)[:-3]+'&nbsp;'+str(current)[-3:]\n",
    "hour=str(pd.to_datetime('now')+pd.to_timedelta('3h'))[11:16]\n",
    "str(szotar['report']['UI'][1:]),current,today0,hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_load=False #load from HU.json - useful when migrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darken2(color):\n",
    "    return '#'+str(hex(int(\"0x\"+color[1:], 16) & 0xfefefe >> 1))[2:]\n",
    "def brighten2(color):\n",
    "    return '#'+str(hex(int(\"0x\"+color[1:], 16) & 0x7f7f7f << 1))[2:]\n",
    "def adjust_lightness(color, amount=0.5):\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    return mc.rgb2hex(colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa={'status':'bar-chart', 'global':'globe','governance':'gavel','stocks':'line-chart','firms':'car','data':'database',\n",
    "    'industry':'industry','exports':'sign-out','credits':'coffee','others':'ellipsis-h','counties':'street-view','macro':'forward',\n",
    "   'dijak':'trophy','mobility':'train','reference':'pencil','imobiliare':'home','news':'newspaper-o'}\n",
    "ids={8:'logo',90:'flogo', 10:'title',6:'main_content',12:'status', 19:'global', 18:'governance', 17:'stocks', 16:'firms', 15:'industry', \n",
    "     14:'counties',121:'footer',57:'footer2',23:'lang',66:'credits',82:'report',85:'others',93:'update',94:'data',98:'macro',\n",
    "    106:'dijak',108:'mobility',13:'reference',124:'imobiliare',144:'news'}\n",
    "plots={21:'governance_plot1',24:'governance_plot2',134:'governance_plot3',22:'global_plot1',32:'social_plot1',37:'social_plot3',36:'social_plot5',\n",
    "      55:'stock',54:'none',42:'stock_plot3',56:'stock_plot4',41:'stock',43:'stock',44:'stock',46:'stock',45:'stock',\n",
    "      48:'stock',49:'stock',50:'stock',51:'stock',53:'stock',61:'firms3',63:'firms5',96:'stock_plot5',95:'stock_plot6',\n",
    "      100:'macro_scatter',110:'mobility_plot1',111:'mobility_plot2',107:'residential_percent_change_from_baseline_plot',\n",
    "      112:'workplaces_percent_change_from_baseline_plot',115:'transit_stations_percent_change_from_baseline_plot',\n",
    "      117:'grocery_and_pharmacy_percent_change_from_baseline_plot',116:'parks_percent_change_from_baseline_plot',\n",
    "      114:'retail_and_recreation_percent_change_from_baseline_plot',132:'imobiliare_plot1',133:'imobiliare_plot2',\n",
    "       127:'imobiliare_plot3',126:'imobiliare_plot4',135:'none',136:'megye_plot1',\n",
    "       137:'megye_plot2',152:'vaccine_plot1',\n",
    "      139:'severity_plot1',140:'severity_plot3'}\n",
    "singlestats={68:'status_plot1',69:'status_plot2',70:'status_plot3',71:'status_plot4',72:'status_plot5',73:'status_plot6',74:'status_plot7',\n",
    "            75:'status_plot14',76:'status_plot9',77:'status_plot10',78:'status_plot11',79:'status_plot12',80:'status_plot13',88:'status_plot10',89:'matrix',\n",
    "            91:'megyecase',99:'GDP growth vs. market and DFM estimates',104:'macro_table',141:'megye_plot3',\n",
    "            109:'mobility_map',125:'imobiliare_map1',128:'imobiliare_map2',138:'severity_plot2',\n",
    "            145:'news1',143:'news3'}\n",
    "heatmaps={59:'firms1',60:'firms2'}\n",
    "tables={31:'global_plot2',33:'social_plot2',39:'social_plot4',38:'social_plot6',62:'firms4',64:'firms6',65:'firms7',146:'news2'}\n",
    "tooltips=[64,65]\n",
    "legends={64:86,65:87}\n",
    "legendtext={legends[i]:'' for i in tooltips}\n",
    "strftimes={'HU':'%Y.%m.%d','RO':'%d/%m/%Y','EN':'%Y-%m-%d'}\n",
    "themes={'dark':{'HU':'Sötét kinézet','RO':'Temă închisă','EN':'Dark theme'},'light':{'HU':'Világos kinézet','RO':'Temă deschisă','EN':'Light theme'}}\n",
    "interactive={'HU':'teljes képernyő','RO':'ecran complet','EN':'fullscreen'}\n",
    "kep={'HU':'kép mentése','RO':'salvează imagine','EN':'save image'}\n",
    "save={'HU':'adatok letöltése','RO':'descarcă datele','EN':'download data'}\n",
    "generate_links=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model,theme):\n",
    "    css=''\n",
    "    texts=[]\n",
    "    for panel in model['panels']:\n",
    "        if 'type' in panel:\n",
    "            panel['description']=''\n",
    "            panel['links']=[]\n",
    "            links=[]\n",
    "            t=''\n",
    "            if theme!='dark':\n",
    "                t='-'+theme+'?theme='+theme\n",
    "            if panel['type']=='text':\n",
    "                texts.append(panel['id'])\n",
    "                if panel['id'] in ids:\n",
    "                    i=ids[panel['id']]\n",
    "                    if i=='logo':\n",
    "                        panel['content']='<a target=\"_blank\" href=\"'+\\\n",
    "                        szotar['logo_link'][lang]+'\"><img id=\"logo\" class=\"logo\" alt=\"'+\\\n",
    "                        szotar['logo_alt'][lang]+'\" src=\"'+htmlepath+\\\n",
    "                        szotar['logo_img'][lang]+'\" style=\"height:70px!important;width:200px!important;\"/></a>'\n",
    "                    elif i=='flogo':\n",
    "                        panel['content']='<a target=\"_blank\" href=\"'+\\\n",
    "                        szotar['flogo_link'][lang]+'\"><img id=\"flogo\" class=\"flogo\" alt=\"'+\\\n",
    "                        szotar['flogo_alt'][lang]+'\" src=\"'+htmlepath+\\\n",
    "                        szotar['flogo_img'][lang]+'\" style=\"height:80px!important;width:330px!important;\"/></a>'\n",
    "                    elif i=='lang':\n",
    "                        panel['content']='<br><div class=\"lang\" style=\"text-align: center; font-size:16px;\">'\n",
    "                        for l in ['RO','HU','EN']:\n",
    "                            panel['content']+='&nbsp;&nbsp;<a target=\"_self\" title=\"'+titles[l]+'\" href=\"/d/'+uids[l]+t+'\">'+l+'</a>'\n",
    "                        panel['content']+='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i class=\"fa fa-paint-brush\"></i>&nbsp;&nbsp;<a target=\"_self\" title=\"'+themes['dark'][lang]+'\" href=\"/d/'+\\\n",
    "                            uids[lang]+'\"><i class=\"fa fa-circle\"></i></a>'+\\\n",
    "                            '&nbsp;&nbsp;<a target=\"_self\" title=\"'+themes['light'][lang]+'\" href=\"/d/'+uids[lang]+'-light?theme=light\"><i class=\"fa fa-circle-o\"></i></a>'\n",
    "                        panel['content']+='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a style=\"font-size:20px;\" target=\"_blank\" title=\"Facebook\" '+\\\n",
    "                        'href=\"https://www.facebook.com/covid19.roeim\"><i class=\"fa fa-facebook\"></i></a>'\n",
    "                        panel['content']+='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a style=\"font-size:20px;\" target=\"_blank\" title=\"Help\" '+\\\n",
    "                        'href=\"https://github.com/denesdata/covid19-romania/wiki\"><i class=\"fa fa-question\"></i></a>'\n",
    "                        panel['content']+='</div>'\n",
    "                    elif i=='title':\n",
    "                        panel['content']='<div style=\"text-align:left;\"><h1>'+str(szotar['title'][lang])+'</h1><h3> '+str(szotar['subtitle'][lang])+'</h3></div>'\n",
    "                    elif i in ['main_content','footer','footer2']:\n",
    "                        panel['content']=str(szotar[i][lang])\n",
    "                    elif i in ['credits','others','data','dijak']:\n",
    "                        panel['content']='<p><h4><i class=\"fa fa-'+fa[i]+'\"></i>&nbsp;&nbsp;'+str(szotar[i][lang])+'</h4><div>'+str(szotar[i+'_content'][lang])+'</div></p>'\n",
    "                    elif i in ['reference']:\n",
    "                        panel['content']='<p><h4><i class=\"fa fa-'+fa[i]+'\"></i>&nbsp;&nbsp;'+str(szotar[i][lang])+'</h4><div>'+str(szotar[i+'_content'][lang])+\\\n",
    "                        ' '+pd.to_datetime('now').strftime(strftimes[lang])+'</div></p>'\n",
    "                    elif i=='report':\n",
    "                        panel['content']=('<p><span style=\"font-size:18px;\">'+str(szotar[i][lang])+'</span></p>')\\\n",
    "                            .replace('$','<span style=\"font-size:26px;color:'+model['panels'][8]['colors'][0]+';\"><b>'+str(szotar[i]['UI'][1:])+'</b></span>')\\\n",
    "                            .replace('+','<span style=\"font-size:26px;color:'+model['panels'][8]['colors'][1]+';\"><b>'+str(current)+'</b></span>')\n",
    "                    elif i=='update':\n",
    "                        panel['content']='<div style=\"text-align:right;\"><i class=\"fa fa-clock-o\"></i>&nbsp;'+str(szotar['update'][lang])+' <b>'+pd.to_datetime('now').strftime(strftimes[lang])+'</b></div>'\n",
    "                    else:\n",
    "                        panel['content']='<p><h2><i class=\"fa fa-'+fa[i]+'\"></i>&nbsp;&nbsp;'+str(szotar[i][lang])+'</h2><div>'+str(szotar[i+'_content'][lang])+'</div></p>'\n",
    "                elif panel['id'] in legendtext:\n",
    "                    panel['content']=legendtext[panel['id']]\n",
    "            elif panel['type']=='graph':\n",
    "                if panel['id'] in plots:\n",
    "                    i=plots[panel['id']]\n",
    "                    if (('stock' in i) or (panel['id'] in [54])):\n",
    "                        panel['timeFrom']= str(int(szotar['report']['UI'][1:])+1)+'d'\n",
    "                        panel['hideTimeOverride']= True\n",
    "                    if i=='stock':\n",
    "                        if lang in ['HU','RO']:\n",
    "                            description=str(szotarHU[panel['title']][lang+'_description'])\n",
    "                            source=str(szotarHU[panel['title']][lang+'_source']) \n",
    "                            panel['title']=szotarHU[panel['title']][lang]\n",
    "                        else:\n",
    "                            description=str(szotarRO[panel['title']][lang+'_description'])\n",
    "                            source=str(szotarRO[panel['title']][lang+'_source'])     \n",
    "                            panel['title']=szotarRO[panel['title']][lang]  \n",
    "                        if description not in ['nan','XXX']:\n",
    "                            panel['description']=description\n",
    "                        if source not in ['nan','XXX']:\n",
    "                            panel['description']+='  \\n📊 '+source\n",
    "                    elif i!='none':\n",
    "                        panel['title']=szotar[i][lang]\n",
    "                        description=str(szotar[i][lang+'_description'])\n",
    "                        source=str(szotar[i][lang+'_source'])    \n",
    "                        if description not in ['nan','XXX']:\n",
    "                            panel['description']=description\n",
    "                        if source not in ['nan','XXX']:\n",
    "                            panel['description']+='  \\n📊 '+source\n",
    "                    for s in panel['seriesOverrides']:\n",
    "                        if 'alias' in s:\n",
    "                            if s['alias'] not in ['/./']:\n",
    "                                if lang in ['HU','RO']:\n",
    "                                    s['alias']=szotarHU[s['alias']][lang]\n",
    "                                else:\n",
    "                                    s['alias']=szotarRO[s['alias']][lang]\n",
    "                        if 'fillBelowTo' in s:\n",
    "                            if lang in ['HU','RO']:\n",
    "                                s['fillBelowTo']=szotarHU[s['fillBelowTo']][lang]\n",
    "                            else:\n",
    "                                s['fillBelowTo']=szotarRO[s['fillBelowTo']][lang]\n",
    "                    for s in panel['targets']:\n",
    "                        if 'alias' in s:\n",
    "                            if lang in ['HU','RO']:\n",
    "                                sz={s:szotarHU[s] for s in szotarHU if szotarHU[s]['UI']=='replace'}\n",
    "                                if s['alias'] in sz:\n",
    "                                    s['alias']=sz[s['alias']][lang]\n",
    "                            else:\n",
    "                                sz={s:szotarRO[s] for s in szotarRO if szotarRO[s]['UI']=='replace'}\n",
    "                                if s['alias'] in sz:\n",
    "                                    s['alias']=szotarRO[s['alias']][lang]\n",
    "\n",
    "            elif panel['type'] in ['singlestat','ryantxu-ajax-panel','heatmap','news']:\n",
    "                if panel['id'] in singlestats:\n",
    "                    if (panel['id'] in [79,80]):\n",
    "                        panel['timeFrom']= str(int(szotar['report']['UI'][1:])+1)+'d'\n",
    "                        panel['hideTimeOverride']= True\n",
    "                    i=singlestats[panel['id']]\n",
    "                    panel['title']=szotar[i][lang]\n",
    "                    description=str(szotar[i][lang+'_description'])\n",
    "                    source=str(szotar[i][lang+'_source'])    \n",
    "                    if description not in ['nan','XXX']:\n",
    "                        panel['description']=description\n",
    "                    if source not in ['nan','XXX']:\n",
    "                        panel['description']+='  \\n📊 '+source\n",
    "                if panel['type'] in ['ryantxu-ajax-panel']:\n",
    "                    panel['url']=panel['url'].replace(htmlepath_other,htmlepath)\n",
    "            elif panel['type'] in ['savantly-heatmap-panel']:\n",
    "                if panel['id'] in heatmaps:\n",
    "                    i=heatmaps[panel['id']]\n",
    "                    if lang in ['HU','RO']:\n",
    "                        panel['title']=szotarHU[panel['title']][lang]\n",
    "                    else:\n",
    "                        panel['title']=szotarRO[panel['title']][lang]\n",
    "                    description=str(szotar[i][lang+'_description'])\n",
    "                    source=str(szotar[i][lang+'_source'])    \n",
    "                    if description not in ['nan','XXX']:\n",
    "                        panel['description']=description\n",
    "                    if source not in ['nan','XXX']:\n",
    "                        panel['description']+='  \\n📊 '+source  \n",
    "            elif panel['type']=='table-old':\n",
    "                if panel['id'] in tables:\n",
    "                    i=tables[panel['id']]\n",
    "                    panel['title']=szotar[i][lang]\n",
    "                    legend='<table class=\"legend\"><tr><td>&nbsp;</td><td>&nbsp;</td></tr>'\n",
    "                    for s in panel['styles']:\n",
    "                        if 'alias' in s:\n",
    "                            if lang in ['HU','RO']:\n",
    "                                sz={s:szotarHU[s] for s in szotarHU if szotarHU[s]['UI']=='replace'}\n",
    "                                if s['alias'] in sz:\n",
    "                                    s['alias']=sz[s['alias']][lang]\n",
    "                            else:\n",
    "                                sz={s:szotarRO[s] for s in szotarRO if szotarRO[s]['UI']=='replace'}\n",
    "                                if s['alias'] in sz:\n",
    "                                    s['alias']=szotarRO[s['alias']][lang]\n",
    "                        if panel['id'] in tooltips:\n",
    "                            if 'link' in s:\n",
    "                                if s['link']:\n",
    "                                    s['linkTooltip']=szotar[s['pattern']][lang]\n",
    "                                    legend+='<tr><td>'+s['alias']+'</td><td>'+szotar[s['pattern']][lang]+'</td></tr>'\n",
    "                    if panel['id'] in tooltips:\n",
    "                        legendtext[legends[panel['id']]]=legend\n",
    "                    description=str(szotar[i][lang+'_description'])\n",
    "                    source=str(szotar[i][lang+'_source'])    \n",
    "                    if description not in ['nan','XXX']:\n",
    "                        panel['description']=description\n",
    "                    if source not in ['nan','XXX']:\n",
    "                        panel['description']+='  \\n📊 '+source\n",
    "            if generate_links:\n",
    "                if panel['type'] in ['graph','ryantxu-ajax-panel','savantly-heatmap-panel']:\n",
    "                    links.append({'targetBlank': True,'title': '🕹 '+interactive[lang],\n",
    "                          'url': '/d/'+uids[lang]+'/?var-lang=$lang&var-theme=$theme&theme=$theme'+\\\n",
    "                                  '&width=900&height=500&tz=Europe%2FBucharest&fullscreen&panelId='+str(panel['id'])+'&${__url_time_range}'})\n",
    "                    links.append({'targetBlank': True,'title': '📷 '+kep[lang],\n",
    "                          'url': '/render/d-solo/'+uids[lang]+'/?var-lang=$lang&var-theme=$theme&theme=$theme'+\\\n",
    "                                  '&width=900&height=500&tz=Europe%2FBucharest&panelId='+str(panel['id'])+'&${__url_time_range}'})\n",
    "            panel['links']=links\n",
    "\n",
    "    model['templating']['list'][0]['current']['text']=lang\n",
    "    model['templating']['list'][0]['current']['value']=lang\n",
    "    model['templating']['list'][1]['current']['text']=theme\n",
    "    model['templating']['list'][1]['current']['value']=theme\n",
    "    for option in model['templating']['list'][0]['options']:\n",
    "        option['selected']=False\n",
    "        if option['value']==lang:\n",
    "            option['selected']=True\n",
    "    if theme=='dark':\n",
    "        model['title']=titles[lang]\n",
    "        model['uid']=uids[lang]\n",
    "        model['id']=iids[lang]\n",
    "        open(lang+'.json','w').write(json.dumps(model))\n",
    "    else:\n",
    "        model['title']=titles[lang]+' Light'\n",
    "        model['uid']=uids_light[lang]\n",
    "        model['id']=iids_light[lang]\n",
    "        open(lang+'-light.json','w').write(json.dumps(model))\n",
    "    for text in texts:\n",
    "        css+='\\n#panel-'+str(text)+' .panel-menu-container {display: none !important;}\\n#panel-'+\\\n",
    "        str(text)+' .panel-title-container {cursor: auto !important;}'\n",
    "    for table in [107,112,115,117,116,114]:\n",
    "        css+='\\n#panel-'+str(table)+' .graph-legend-table .graph-legend-icon {display: none;}\\n#panel-'+\\\n",
    "        str(table)+' .graph-legend-table td{padding: 1px 2px;}'\n",
    "    for table in [107,112,115,117,116,114]:\n",
    "        css+='\\n#panel-'+str(table)+' .graph-legend-table thead {display: none;}\\n#panel-'+\\\n",
    "        str(table)+' .graph-legend-table tr td:first-child{width:80px !important;display: grid;}'\n",
    "    for table in [110,111]:\n",
    "        css+='\\n#panel-'+str(table)+' .graph-legend-table tr td:first-child{width:190px !important;display: inline-block;}'\n",
    "    open(htmlipath+'/style/grafana/custom.'+theme+'.css','w').write(css)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# szotardf,szotar,szotarHU,szotarRO,szotarEN=get_szotar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(grafana+'api/dashboards/uid/'+uids['HU'], headers=headers)\n",
    "\n",
    "response = requests.get(grafana+'api/dashboards/uid/hu', headers=headers)\n",
    "\n",
    "if local_load:\n",
    "    model_dark=json.loads(open('HU.json','r').read().replace(htmlepath_other,htmlepath))\n",
    "else:\n",
    "    model_dark=json.loads(response.content)['dashboard']\n",
    "\n",
    "# model_dark['time']['from']='now-'+str(szotar['report']['UI'][1:])+'d'\n",
    "model_dark['time']['from']='now-'+str((pd.to_datetime('now')-pd.to_datetime('2020-03-13')).days)+'d'\n",
    "model_dark['time']['to']='now+14d'\n",
    "model_light=json.dumps(model_dark).replace('dark.css','light.css').replace('lightGray','#52545C')\n",
    "colors_to_darken=['#F2CC0C','#CA95E5','#FF780A','#E0B400','#96D98D','#F2495C','#E02F44',\n",
    "                  'lime','#73BF69','#8AB8FF','#3274D9','#D3D3D3','#FF7383','#FADE2A']\n",
    "for color in colors_to_darken:\n",
    "    model_light=model_light.replace(color,adjust_lightness(color,0.8))\n",
    "    model_light=model_light.replace(color.lower(),adjust_lightness(color,0.8))\n",
    "model_light=json.loads(model_light)\n",
    "models={'dark':model_dark,'light':model_light}\n",
    "menu_sections={uids[i]:'' for i in languages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in model_dark['panels']:\n",
    "#     print(i['id'],i['type'],i['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_menus(model,lang):\n",
    "    menu_sections[uids[lang]]='<table style=\"line-height:XRXRLHpx\" class=\"XRXRCLASS\"><tr>'+\\\n",
    "    '<td style=\"width: auto;\" onclick=\"scroll_nicely(\\'.dashboard-container\\')\">'+\\\n",
    "    '<div><i class=\"fa fa-arrow-circle-up\"></i></div><div class=\"nav-text\">'+str(szotar['start'][lang])+'</div></td>'\n",
    "    for panel in model['panels']:\n",
    "        if 'type' in panel:\n",
    "            if panel['type']=='text':\n",
    "                if panel['id'] in ids:\n",
    "                    i=ids[panel['id']]\n",
    "                    if 'content' in panel:\n",
    "                        if (('<h2>' in panel['content']) or (i in ['credits'])):\n",
    "                            m='<td onclick=\"scroll_nicely(\\'#panel-'+str(panel['id'])+\\\n",
    "                            '\\')\"><div><i class=\"fa fa-'+fa[i]+'\"></i></div><div class=\"nav-text\">'+\\\n",
    "                                str(szotar[i][lang])+'</div></td>'\n",
    "                            menu_sections[uids[lang]]+=m\n",
    "    menu_sections[uids[lang]]+='</tr></table>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{\"id\":2,\"slug\":\"magyar\",\"status\":\"success\",\"uid\":\"hu\",\"url\":\"/d/hu/magyar\",\"version\":279}'\n",
      "<Response [200]> b'{\"id\":5,\"slug\":\"magyar-light\",\"status\":\"success\",\"uid\":\"hu-light\",\"url\":\"/d/hu-light/magyar-light\",\"version\":122}'\n",
      "<Response [200]> b'{\"id\":3,\"slug\":\"romana\",\"status\":\"success\",\"uid\":\"ro\",\"url\":\"/d/ro/romana\",\"version\":183}'\n",
      "<Response [200]> b'{\"id\":6,\"slug\":\"romana-light\",\"status\":\"success\",\"uid\":\"ro-light\",\"url\":\"/d/ro-light/romana-light\",\"version\":116}'\n",
      "<Response [200]> b'{\"id\":4,\"slug\":\"english\",\"status\":\"success\",\"uid\":\"en\",\"url\":\"/d/en/english\",\"version\":152}'\n",
      "<Response [200]> b'{\"id\":7,\"slug\":\"english-light\",\"status\":\"success\",\"uid\":\"en-light\",\"url\":\"/d/en-light/english-light\",\"version\":117}'\n"
     ]
    }
   ],
   "source": [
    "nevuto={'0':'á','1':'é','3':'á','2':'á','4':'é','5':'é','6':'á','7':'é','8':'á','9':'é'}\n",
    "for lang in languages:\n",
    "    response = requests.get(grafana+'api/dashboards/uid/'+lang.lower(), headers=headers)\n",
    "    model=json.loads(response.content)['dashboard']\n",
    "    for i in model['panels']:\n",
    "        if lang=='HU':\n",
    "            #print(i['id'],i['type'],i['title'])\n",
    "            if i['id'] in (179, 91):\n",
    "                i['options']['iframeURL']=i['options']['iframeURL'][:i['options']['iframeURL'].find('.html#')-2]+\\\n",
    "                    today0+i['options']['iframeURL'][i['options']['iframeURL'].find('.html#'):]\n",
    "                i['description']=i['description'][:i['description'].find('n,')-4].strip()+' '+today+'-'+\\\n",
    "                    nevuto[now[-1]]+i['description'][i['description'].find('n,'):\\\n",
    "                    i['description'].find(':')-2]+hour+\\\n",
    "                    i['description'][i['description'].find(':')+3:]\n",
    "                i['title']=i['title'][:i['title'].rfind(' ')]+' '+today\n",
    "            if i['id']==82:\n",
    "                i['content']=i['content'][:i['content'].find('<b>')+3]+str(szotar['report']['UI'][1:])+\\\n",
    "                    i['content'][i['content'].find('</b>'):i['content'].rfind('<b>')+3]+currents+\\\n",
    "                    i['content'][i['content'].rfind('</b>'):]\n",
    "            if i['id']==93:\n",
    "                i['content']=i['content'][:i['content'].rfind('</b>')-2]+today0+\\\n",
    "                i['content'][i['content'].rfind('</b>'):]\n",
    "        elif lang=='EN':\n",
    "            if i['id'] in (153, 91):\n",
    "                i['options']['iframeURL']=i['options']['iframeURL'][:i['options']['iframeURL'].find('.html#')-2]+\\\n",
    "                    today0+i['options']['iframeURL'][i['options']['iframeURL'].find('.html#'):]\n",
    "                i['description']=i['description'][:i['description'].find(':')-2]+hour+' on '+today+' '+ i['description'][i['description'].find('on ')+5:]\n",
    "                i['title']=i['title'][:i['title'].find('📆')+2]+today+i['title'][i['title'].find('📆')+3:]\n",
    "            if i['id']==82:\n",
    "                i['content']=i['content'][:i['content'].find('<b>')+3]+str(szotar['report']['UI'][1:])+\\\n",
    "                    i['content'][i['content'].find('</b>'):i['content'].rfind('<b>')+3]+currents+\\\n",
    "                    i['content'][i['content'].rfind('</b>'):]\n",
    "            if i['id']==93:\n",
    "                i['content']=i['content'][:i['content'].rfind('</b>')-2]+today0+\\\n",
    "                i['content'][i['content'].rfind('</b>'):]\n",
    "        elif lang=='RO':\n",
    "            if i['id'] in (153, 91):\n",
    "                i['options']['iframeURL']=i['options']['iframeURL'][:i['options']['iframeURL'].find('.html#')-2]+\\\n",
    "                    today0+i['options']['iframeURL'][i['options']['iframeURL'].find('.html#'):]\n",
    "                i['description']=i['description'][:i['description'].find(':')-2]+hour+' în data de '+today+' '+ i['description'][i['description'].find('în data de ')+13:]\n",
    "                i['title']=i['title'][:i['title'].find('📆')+2]+today+i['title'][i['title'].find('📆')+3:]\n",
    "            if i['id']==82:\n",
    "                i['content']=i['content'][:i['content'].find('<b>')+3]+str(szotar['report']['UI'][1:])+\\\n",
    "                    i['content'][i['content'].find('</b>'):i['content'].rfind('<b>')+3]+currents+\\\n",
    "                    i['content'][i['content'].rfind('</b>'):]\n",
    "            if i['id']==93:\n",
    "                i['content']=i['content'][:i['content'].rfind('<b>')+3]+today0+\\\n",
    "                i['content'][i['content'].rfind('<b>')+5:]\n",
    "            \n",
    "    r=requests.post(grafana+'api/dashboards/db', headers=headers, json={\"dashboard\":model,\n",
    "                                                                            \"folderId\": folder_id,\n",
    "                                                                            \"overwrite\": True\n",
    "                                                                           })       \n",
    "    print(r,r.content)\n",
    "    open(lang+'.json','w').write(json.dumps(model))    \n",
    "    \n",
    "    model_light=json.dumps(model).replace('dark','light',9999).replace('lightGray','#52545C',9999)\\\n",
    "        .replace('#d3d3d3','#52545c',9999)\\\n",
    "        .replace('csaladen.es/favicon.ico\" style=\"','csaladen.es/favicon.ico\" style=\"filter: brightness(0.3);',9999)\n",
    "    for color in colors_to_darken:\n",
    "        model_light=model_light.replace(color,adjust_lightness(color,0.8),9999)\n",
    "        model_light=model_light.replace(color.lower(),adjust_lightness(color,0.8),9999)\n",
    "    model=json.loads(model_light)\n",
    "    model['title']=titles[lang]+' Light'\n",
    "    model['uid']=uids_light[lang]\n",
    "    model['id']=iids_light[lang]\n",
    "    r=requests.post(grafana+'api/dashboards/db', headers=headers, json={\"dashboard\":model,\n",
    "                                                                            \"folderId\": folder_id,\n",
    "                                                                            \"overwrite\": True\n",
    "                                                                           })    \n",
    "    print(r,r.content)\n",
    "    open(lang+'-light.json','w').write(json.dumps(model))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX LIGHT THEME CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dcss=open(htmlipath+'/style/grafana/dark.css','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcss=dcss.replace('#243b55','#abcaee',9999)\\\n",
    ".replace('lightGray','rgb(82, 84, 92)',9999)\\\n",
    ".replace('#141e30','#ecf2ff',9999)\\\n",
    ".replace('rgba(0, 0, 0, 0.6)','rgba(255, 255, 255, 0.6)',9999)\\\n",
    ".replace('#60c5eb','#86b2f6',9999)\\\n",
    ".replace('#d8d9da','#52545c',9999)\\\n",
    ".replace('#d3d3d3','#52545c',9999)\\\n",
    ".replace('#161719','#f7f8fa',9999)\\\n",
    ".replace('#fff','#1e2028',9999)\\\n",
    ".replace('#343436','#c7d0d9',9999)\\\n",
    ".replace('#8e8e8e','#767980',9999)\\\n",
    ".replace('#b3b3b3','#acb6bf',9999)\\\n",
    ".replace('#262628','#dde4ed',9999)\\\n",
    ".replace('#33b5e5','#5794f2',9999)\\\n",
    ".replace('#60c5eb','#86b2f6',9999)\\\n",
    ".replace('#ecbb13','#ff851b',9999)\\\n",
    ".replace('#0b0c0e','#fff',9999)\\\n",
    ".replace('#c7d0d9','#41444b',9999)\\\n",
    ".replace('#ececec','#41444b',9999)\\\n",
    ".replace('#97999c','#767980',9999)\\\n",
    ".replace('#1C2D43','#cedff4',9999)\\\n",
    ".replace('checkbox.png','checkbox_white.png',9999)\\\n",
    ".replace('#e9edf2','#141414',9999)\\\n",
    ".replace('icons_dark_theme','icons_light_theme',9999)\\\n",
    ".replace('#d8d9da','#f7f8fa',9999)\\\n",
    ".replace('#000','#52545c',9999)\\\n",
    ".replace('brightness(50)','brightness(1)',9999)\\\n",
    ".replace('dark','light',9999)+' body, a{color:#52545c !important;} .credit a, .panel-info-content a, .tooltip {color: lightGray !important;}'+\\\n",
    "    ' .flogo {filter: brightness(0.5);}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481214"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'/style/grafana/light.css','w').write(lcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for theme in ['dark','light']:\n",
    "#     for lang in languages:\n",
    "#         print(theme,lang)  \n",
    "#         model=get_model(models[theme],theme)\n",
    "#         if theme=='dark':\n",
    "#             add_to_menus(model,lang)\n",
    "#         r=requests.post(grafana+'api/dashboards/db', headers=headers, json={\"dashboard\":model,\n",
    "#                                                                         \"folderId\": folder_id,\n",
    "#                                                                         \"overwrite\": True\n",
    "#                                                                        })\n",
    "#         print(r,r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menu_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart `InfluxDB` and `Grafana` to proceed to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_annoations=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_time=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_annotations(tag, lang):\n",
    "    if process_annoations:\n",
    "        for iid in [iids, iids_light]:\n",
    "            time.sleep(1)\n",
    "            dash=iid[lang]\n",
    "            if tag=='all':\n",
    "                r=requests.get(grafana+'api/annotations?limit=10000&dashboardId='+\n",
    "                               str(dash), headers=headers)\n",
    "            else:\n",
    "                r=requests.get(grafana+'api/annotations?limit=10000&tags='+tag+\n",
    "                               '&dashboardId='+str(dash), headers=headers)\n",
    "            annotations=json.loads(r.content)\n",
    "            print('Expected running time for query is:',int(len(annotations)*sleep_time*2),'seconds.')\n",
    "            for a in annotations:\n",
    "                time.sleep(sleep_time)\n",
    "                r=requests.delete(grafana+'api/annotations/'+str(a['id']), headers=headers)\n",
    "            print(lang,dash,len(annotations),'annotations purged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_annotations(a,lang):\n",
    "    if process_annoations:\n",
    "        for iid in [iids, iids_light]:\n",
    "            time.sleep(sleep_time)\n",
    "            a[\"dashboardId\"]=iid[lang]\n",
    "            requests.post(grafana+'api/annotations', headers=headers, json=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "forras={'HU':'forrás','RO':'sursa','EN':'source'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "if purge:\n",
    "    for lang in ['HU','RO','EN']:\n",
    "        purge_annotations('all',lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='global'\n",
    "df=pd.read_csv(url+sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU\n",
      "RO\n",
      "EN\n"
     ]
    }
   ],
   "source": [
    "mtag='global'\n",
    "for lang in ['HU','RO','EN']:\n",
    "    purge_annotations(mtag,lang)\n",
    "    for i in df.T.to_dict().values():\n",
    "        tags=[mtag]\n",
    "        if str(i['tag1'])!='nan':\n",
    "            tags.append(szotar[str(i['tag1'])][lang])\n",
    "        if str(i['tag2'])!='nan':\n",
    "            tags.append(szotar[str(i['tag2'])][lang])\n",
    "        a={\n",
    "          \"panelId\":22,\n",
    "          \"time\":int(pd.to_datetime(i['Dátum']).strftime(\"%s\"))*1000,\n",
    "          # \"timeEnd\":int((pd.to_datetime(i['Dátum'])+pd.to_timedelta('9h')).strftime(\"%s\"))*1000,\n",
    "          \"tags\":tags,\n",
    "          \"text\":'<div class=\"annotation_\"'+mtag+'>'+i[lang]+'</div><a target=\"_blank\" href=\"'+i['Link']+'\">'+forras[lang]+'</a>',\n",
    "        }\n",
    "        post_annotations(a,lang)\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governance - economic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='governance'\n",
    "df=pd.read_csv(url+sheet)\n",
    "dc=df.dropna(subset=['gov_note_econ'])[['gov_note_econ','gov_note_econ_tag','date','law_link']]\n",
    "dc['date']=resolve_time_conflicts(dc['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU\n",
      "RO\n",
      "EN\n"
     ]
    }
   ],
   "source": [
    "mtag='economic'\n",
    "for lang in ['HU','RO','EN']:\n",
    "    purge_annotations(mtag, lang)\n",
    "    for i in dc.T.to_dict().values():\n",
    "        tags=[mtag]\n",
    "        if str(i['gov_note_econ_tag'])!='nan':\n",
    "            tags.append(szotar[str(i['gov_note_econ_tag'])][lang])\n",
    "        a={\n",
    "          \"panelId\":32,\n",
    "          \"time\":int(pd.to_datetime(i['date']).strftime(\"%s\"))*1000,\n",
    "          # \"timeEnd\":int((pd.to_datetime(i['date'])+pd.to_timedelta('9h')).strftime(\"%s\"))*1000,\n",
    "          \"tags\":tags,\n",
    "          \"text\":'<div class=\"annotation_\"'+mtag+'>'+szotar[i['gov_note_econ']][lang]+\\\n",
    "            '</div><a target=\"_blank\" href=\"'+i['law_link']+'\">'+forras[lang]+'</a>',\n",
    "        }\n",
    "        post_annotations(a,lang)\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governance - social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='governance'\n",
    "df=pd.read_csv(url+sheet)\n",
    "dc=df.dropna(subset=['gov_note_social'])[['gov_note_social','gov_note_social_tag','date','law_link']]\n",
    "dc['date']=resolve_time_conflicts(dc['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU\n",
      "RO\n",
      "EN\n"
     ]
    }
   ],
   "source": [
    "mtag='social'\n",
    "for lang in ['HU','RO','EN']:\n",
    "    purge_annotations(mtag, lang)\n",
    "    for i in dc.T.to_dict().values():\n",
    "        tags=[mtag]\n",
    "        if str(i['gov_note_social_tag'])!='nan':\n",
    "            tags.append(szotar[str(i['gov_note_social_tag'])][lang])\n",
    "        a={\n",
    "          \"panelId\":37,\n",
    "          \"time\":int(pd.to_datetime(i['date']).strftime(\"%s\"))*1000,\n",
    "          # \"timeEnd\":int((pd.to_datetime(i['date'])+pd.to_timedelta('9h')).strftime(\"%s\"))*1000,\n",
    "          \"tags\":tags,\n",
    "          \"text\":'<div class=\"annotation_\"'+mtag+'>'+szotar[i['gov_note_social']][lang]+\\\n",
    "            '</div><a target=\"_blank\" href=\"'+i['law_link']+'\">'+forras[lang]+'</a>',\n",
    "        }\n",
    "        post_annotations(a,lang)\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governance - financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='governance'\n",
    "df=pd.read_csv(url+sheet)\n",
    "dc=df.dropna(subset=['gov_note_fin'])[['gov_note_fin','fin_tag','date','fin_body','fin_link']]\n",
    "dc['date']=resolve_time_conflicts(dc['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU\n",
      "RO\n",
      "EN\n"
     ]
    }
   ],
   "source": [
    "mtag='financial'\n",
    "for lang in ['HU','RO','EN']:\n",
    "    purge_annotations(mtag, lang)\n",
    "    for i in dc.T.to_dict().values():\n",
    "        tags=[mtag]\n",
    "        if str(i['fin_body'])!='nan':\n",
    "            tags.append(szotar[str(i['fin_body'])][lang])\n",
    "        if str(i['fin_tag'])!='nan':\n",
    "            tags.append(szotar[str(i['fin_tag'])][lang])\n",
    "        a={\n",
    "          \"panelId\":36,\n",
    "          \"time\":int(pd.to_datetime(i['date']).strftime(\"%s\"))*1000,\n",
    "          # \"timeEnd\":int((pd.to_datetime(i['date'])+pd.to_timedelta('9h')).strftime(\"%s\"))*1000,\n",
    "          \"tags\":tags,\n",
    "          \"text\":'<div class=\"annotation_\"'+mtag+'>'+szotar[i['gov_note_fin']][lang]+\\\n",
    "            '</div><a target=\"_blank\" href=\"'+i['fin_link']+'\">'+forras[lang]+'</a>',\n",
    "        }\n",
    "        post_annotations(a,lang)\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='events'\n",
    "df=pd.read_csv(url+sheet)\n",
    "dc=df[['Friendly name','Date','HU', 'RO','EN','Link', 'Link.1', 'Link.2']]\n",
    "dc.columns=['name','date','HU','RO','EN','link1','link2','link3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='firms'\n",
    "df=pd.read_csv(url+sheet)\n",
    "de=df[['Friendly name', 'RO CAEN sectiune','HU CAEN sectiune', 'EN CAEN sectiune']]\n",
    "de.columns=['name','RO', 'HU','EN']\n",
    "de=de.set_index('name').stack().reset_index()\n",
    "de.columns=['name','lang','industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc=dc.set_index(['name','date','link1','link2','link3']).stack().reset_index()\n",
    "dc.columns=['name','date','link1','link2','link3','lang','steps']\n",
    "dc=dc.set_index(['name','lang']).join(de.set_index(['name','lang'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "annos1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_annoations:\n",
    "    for lang in ['HU','RO','EN']:\n",
    "        purge_annotations('firms', lang)\n",
    "        purge_annotations('facebook', lang)\n",
    "        dc3=dc[dc['lang']==lang]\n",
    "        ds={}\n",
    "        for i in dc3.T.to_dict().values():\n",
    "            links=''\n",
    "            links1=['']\n",
    "            mtag='firms'\n",
    "            pid=61\n",
    "            atag=szotar['vallalati'][lang]\n",
    "            for link in ['link1','link2','link3']:\n",
    "                l=str(i[link])\n",
    "                if l!='nan':\n",
    "                    if links=='':\n",
    "                        links='<a target=\"_blank\" href=\"'+l+'\">'+forras[lang]+'</a>'\n",
    "                        links1[0]=l\n",
    "                        if 'facebook' in l:\n",
    "                            mtag='facebook'\n",
    "                            pid=63\n",
    "                            atag='Facebook'\n",
    "                    else:\n",
    "                        links+='; <a target=\"_blank\" href=\"'+l+'\">'+forras[lang]+'</a>'\n",
    "                        links1.append(l)\n",
    "            tags=[mtag]\n",
    "            if str(i['name'])!='nan':\n",
    "                tags.append(str(i['name']))\n",
    "            if str(i['industry'])!='nan':\n",
    "                tags.append(str(i['industry']))\n",
    "            d=str(i['date'])\n",
    "            if d=='nan':\n",
    "                tags.append(str('no date'))\n",
    "                d='2020-03-10'\n",
    "            if d not in ds:\n",
    "                ds[d]=pd.to_datetime(d)\n",
    "                t=(ds[d])\n",
    "            else:\n",
    "                ds[d]=ds[d]+pd.to_timedelta('193m')\n",
    "                t=(ds[d])\n",
    "            a={\n",
    "              \"panelId\":pid,\n",
    "              \"time\":int(t.strftime(\"%s\"))*1000,\n",
    "              # \"timeEnd\":int((pd.to_datetime(t)+pd.to_timedelta('9h')).strftime(\"%s\"))*1000,\n",
    "              \"tags\":tags,\n",
    "              \"text\":'<div><b>'+str(i['name'])+'</b></div><div class=\"annotation_\"'+mtag+'>'+i['steps']+\\\n",
    "                '</div>'+links,\n",
    "            }\n",
    "            post_annotations(a,lang)\n",
    "            annos1.append({'name':str(i['name']),'date':t,'event':i['steps'],'link':links1[0],'type':atag,'lang':lang})\n",
    "        print(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push annos to Influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_annoations:\n",
    "    da=pd.DataFrame(annos1).set_index('date')\n",
    "    da['event']=da['event'].str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_annoations:\n",
    "    da3=[]\n",
    "    for lang in ['HU','RO','EN']:\n",
    "        da2=da[da['lang']==lang].sort_index(ascending=False)\n",
    "        ds={}\n",
    "        ts=[]\n",
    "        for d in da2.index:\n",
    "            if d not in ds:\n",
    "                ds[d]=pd.to_datetime(d)\n",
    "                t=(ds[d])\n",
    "            else:\n",
    "                ds[d]=ds[d]+pd.to_timedelta('193m')\n",
    "                t=(ds[d])\n",
    "            ts.append(t)\n",
    "        da2.index=ts\n",
    "        da3.append(da2)\n",
    "    da3=pd.concat(da3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_annoations:\n",
    "    field_columns=['name','event','link','type']\n",
    "    tag_columns=['lang']\n",
    "    measurement='firms2'\n",
    "    push2influx(da3,measurement,field_columns,tag_columns,shift=False,wo=process_annoations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Industry county map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iso_counties=pd.read_html('https://en.wikipedia.org/wiki/Counties_of_Romania')[1]\\\n",
    "#     .set_index(['ISO code[note 3]'])['County'].to_dict()\n",
    "# iso_counties['BN']='Bistrița-Năsăud'\n",
    "# iso_counties['CL']='Călărași'\n",
    "# open(htmlipath+'panels/iso_counties.json','w').write(json.dumps(iso_counties))\n",
    "iso_counties=json.loads(open(htmlipath+'panels/iso_counties.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "georo=json.loads(open(htmlipath+'panels/romania-counties.json','r').read())\n",
    "georoco={i['properties']['NAME_1']:i['properties']['ID_1'] for i in georo['objects']['ROU_adm1']['geometries']}   \n",
    "georoco['București']=georoco['Bucharest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='industry_county'\n",
    "df=pd.read_csv(url+sheet)[:-2]\n",
    "df=df[df.columns[:5]]\n",
    "df.columns=['HU','RO','EN','q2','q3']\n",
    "for q in ['q2','q3']:\n",
    "    df[q]=df[q].str.replace('%','').astype(float)\n",
    "df=df.set_index('RO').join(pd.DataFrame(georoco,index=['county']).T).reset_index().dropna()\n",
    "df.columns=['RO','HU','EN','q2','q3','county']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-531-1a3a71a98fd4>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ddd[c+'_high']=True\n",
      "<ipython-input-531-1a3a71a98fd4>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ddd[c+'_high']=False\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    dc=df[[lang,'q2','q3','county']]\n",
    "    dc.columns=['county','q2','q3','id']\n",
    "    for c in ['q2','q3']:\n",
    "        dd=dc.set_index([c])[['county']].sort_index(ascending=False)\n",
    "        dds=[]\n",
    "        ddd=dd.iloc[:top]\n",
    "        ddd[c+'_high']=True\n",
    "        dds.append(ddd)\n",
    "        ddd=dd.iloc[-top:]\n",
    "        ddd[c+'_high']=False\n",
    "        dds.append(ddd)\n",
    "        dds=pd.concat(dds)\n",
    "        dds[c+'_display']=True\n",
    "        dc=dc.set_index(['county']).join(dds.reset_index().set_index(['county'])\\\n",
    "                                         [[c+'_display',c+'_high']]).reset_index()\n",
    "        dc[c+'_display']=dc[c+'_display'].fillna(False)\n",
    "        \n",
    "    dcqs=[]\n",
    "    for c in ['q2','q3']:\n",
    "        dcq=dc.set_index([c,'county'])[[c+'_display',c+'_high']]\n",
    "        dcq.index.names=['value','county']\n",
    "        dcq['quarter']=c\n",
    "        dcq=dcq.reset_index().set_index(['quarter','county','value'])\n",
    "        dcq.columns=['display','high']\n",
    "        dcq['high']=dcq['high'].fillna(False)\n",
    "        dcqs.append(dcq)\n",
    "    dcqs=pd.concat(dcqs)\n",
    "    dcc=dc.set_index(['county','id'])[['q2','q3']].stack().reset_index()\n",
    "    dcc.columns=['county','id','quarter','value']\n",
    "    dcc=dcc.set_index(['quarter','county','value']).join(dcqs).reset_index()\n",
    "    dcc['q']=dcc['quarter'].replace({'q2':'Q2 vs Q1','q3':'Q3 vs Q2'})\n",
    "    dcc['q0']=dcc['quarter'].replace({'q2':2019,'q3':2020})\n",
    "    # dcc=dcc.sort_values('quarter',ascending=False)\n",
    "    open(htmlipath+'panels/county_'+lang+'.json','w').write(json.dumps(list(dcc.T.to_dict().values())))\n",
    "    # open('county_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case county map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='counties_current'\n",
    "dg=pd.read_csv(url+sheet, header=None)\n",
    "dgl=dg[[1,4]].set_index(4).T.to_dict()\n",
    "dgl['Dambovita']=dgl['Dâmbovita']\n",
    "dgl['Szucsáva']=dgl['Suceava']\n",
    "dgl['Galac']=dgl['Galati']\n",
    "dgl['Valcea']=dgl['Vâlcea']\n",
    "dfl=df[languages+['county']].set_index('HU')\n",
    "dh=dfl.join(pd.DataFrame(dgl).T).reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    dc=dh[[lang,1,'county']]\n",
    "    dc.columns=['county','value','id']\n",
    "    open(htmlipath+'panels/county2_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))\n",
    "    # open('county2_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case county map fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop=json.loads(open(htmlipath+'panels/pop.json').read())\n",
    "pop={i:pop[i][1] for i in pop}\n",
    "pdf=pd.DataFrame(pop,index=['pop']).T\n",
    "pdf.index.name='iso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq=pd.DataFrame(dlz_counties).groupby('date').mean().drop('-',axis=1).stack().reset_index()\n",
    "dq.columns=['date','iso','cases']\n",
    "iso2=pd.DataFrame(iso_counties,index=['EN']).T.reset_index()\n",
    "iso2.columns=['iso','EN']\n",
    "dq=dq.set_index('iso').join(pdf).join(iso2.set_index('iso')).set_index('EN').join(dh.set_index('EN').drop(1,axis=1)).reset_index()\n",
    "dq['case_cap']=dq['cases']*1000/dq['pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-536-0c1092ac59e8>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['county']='d'\n",
      "<ipython-input-536-0c1092ac59e8>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['case_14_cap']=4\n",
      "<ipython-input-536-0c1092ac59e8>:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['id']=99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-536-0c1092ac59e8>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['county']='d'\n",
      "<ipython-input-536-0c1092ac59e8>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['case_14_cap']=4\n",
      "<ipython-input-536-0c1092ac59e8>:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['id']=99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-536-0c1092ac59e8>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['county']='d'\n",
      "<ipython-input-536-0c1092ac59e8>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['case_14_cap']=4\n",
      "<ipython-input-536-0c1092ac59e8>:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d['id']=99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN\n"
     ]
    }
   ],
   "source": [
    "dmss=[]\n",
    "for lang in languages:\n",
    "    dms=[]\n",
    "    # dm=dq.set_index([lang,'date'])[['cases','case_cap','pop','county']]\n",
    "    dm=dq.set_index('EN').join(iso2.set_index('EN')).reset_index().set_index([lang,'date'])[['cases','case_cap','pop','county','iso']]\n",
    "    for m in dm.index.get_level_values(0).unique():\n",
    "        dc=dm.loc[m].sort_index()\n",
    "        p=dc.iloc[0]['pop']\n",
    "        dc['case_14']=dc['cases'].diff().rolling(14).sum()\n",
    "        dc['case_14_cap']=dc['case_14']*1000/p\n",
    "        dc['id']=dc['county']\n",
    "        dc['county']=m\n",
    "        dms.append(dc.dropna())\n",
    "    dms=pd.concat(dms)\n",
    "    dms=dms[((dms['case_14_cap']<14)&(dms['case_14_cap']>0))] #filter out non-sense values\n",
    "    open(htmlipath+'panels/county2c_'+lang+'.json','w').write(json.dumps(list(dms.reset_index().T.to_dict().values())))\n",
    "    dms2=dms.sort_index()['2020-08-05':]\n",
    "    open(htmlipath+'panels/county2d_'+lang+'.json','w').write(json.dumps(list(dms2.reset_index().T.to_dict().values())))\n",
    "    \n",
    "    dc=dms.reset_index().sort_index(ascending=False)\n",
    "    for c in ['cases','case_cap','case_14','case_14_cap']:\n",
    "        dd=dc.reset_index().set_index(['date',c])[['county']].sort_index(ascending=False)\n",
    "        dds=[]\n",
    "        for d in dd.index.get_level_values(0).unique():\n",
    "            dds.append(dd.loc[[d]].iloc[:top])\n",
    "            dds.append(dd.loc[[d]].iloc[-top:])\n",
    "        dds=pd.concat(dds)\n",
    "        dds[c+'_display']=True\n",
    "        dc=dc.set_index(['date','county']).join(dds.reset_index().set_index(['date','county'])[c+'_display']).reset_index()\n",
    "        dc[c+'_display']=dc[c+'_display'].fillna(False)\n",
    "        \n",
    "    d=dc.iloc[[0]]\n",
    "    d['county']='d'\n",
    "    d['case_14_cap']=4\n",
    "    d['id']=99\n",
    "        \n",
    "    open(htmlipath+'panels/county2b_'+lang+'.json','w').write(json.dumps(list(d.T.to_dict().values())+list(dc.T.to_dict().values())))\n",
    "    open(htmlipath+'panels/daily/county2b_'+latest+'_'+lang+'.json','w').write(json.dumps(list(d.T.to_dict().values())+list(dc.T.to_dict().values())))\n",
    "    dms['lang']=lang\n",
    "    dmss.append(dms)\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15941"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_counties2=open(htmlipath+'panels/cases_counties2.html','r').read()\n",
    "cases_counties2=cases_counties2[:cases_counties2.find('daily/county2b_\\' + \\'2021-')+20]+latest+cases_counties2[cases_counties2.find('daily/county2b_\\' + \\'2021-')+30:]\n",
    "open(htmlipath+'panels/cases_counties2.html','w').write(cases_counties2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counties per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz=dh.set_index('EN').join(pd.DataFrame(pd.DataFrame(iso_counties,index=['county'])\\\n",
    "                                     .T.reset_index().set_index('county'))).reset_index()\n",
    "dlz_data=pd.DataFrame(pd.DataFrame(dlz_counties).groupby('date').mean().stack()).reset_index()\n",
    "dlz_data.columns=['date','index','value']\n",
    "dz=dlz_data.set_index('index').join(dz.set_index('index')).dropna().set_index('date')\n",
    "dz.index=pd.to_datetime(dz.index)\n",
    "dz2=dz.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dz2=dz2.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz=pd.DataFrame(dz.set_index(['county','value',1],append=True).stack()).reset_index()\n",
    "dz.columns=['date','county','value','dump','lang','langtype']\n",
    "dz=dz.set_index('date').drop('dump',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing counties from 2021-04-02 00:00:00+00:00 ...\n",
      "Writing to counties ...\n",
      "504 data points will be written in 0.1008 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langtype']\n",
    "measurement='counties'\n",
    "push2influx(dz,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counties active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz2=dz2.reset_index().set_index(['EN','HU','RO','county',1,'date'])\\\n",
    "    .unstack().diff(axis=1).stack().reset_index().set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz2=pd.DataFrame(dz2.set_index(['county','value',1],append=True).stack()).reset_index()\n",
    "dz2.columns=['date','county','value','dump','lang','langtype']\n",
    "dz2=dz2.set_index('date').drop('dump',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing counties2 from 2021-04-02 00:00:00+00:00 ...\n",
      "Writing to counties2 ...\n",
      "504 data points will be written in 0.1008 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langtype']\n",
    "measurement='counties2'\n",
    "push2influx(dz2,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County intensity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmsr=pd.concat(dmss)[['case_14_cap','county','iso','lang']]\n",
    "dmsr.index=pd.to_datetime(dmsr.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing counties3 from 2021-04-02 00:00:00+00:00 ...\n",
      "Writing to counties3 ...\n",
      "504 data points will be written in 0.1008 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['case_14_cap']\n",
    "tag_columns=['county','iso','lang']\n",
    "measurement='counties3'\n",
    "push2influx(dmsr,measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County population density correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpdc=dmsr.groupby('iso').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_counties=pd.read_html('https://en.wikipedia.org/wiki/Counties_of_Romania')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "insse_pop=pd.read_csv(htmlipath+'panels/exportPivot_POP106A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_counties['County2']=wiki_counties['County'].str.replace('ș','s').str.replace('ă','a')\\\n",
    "    .str.replace('ț','t').str.replace('â','a').str.replace('Bucharest','Municipiul Bucuresti')\n",
    "insse_pop['County2']=insse_pop[' Macroregiuni  regiuni de dezvoltare si judete'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_counties=wiki_counties.join(insse_pop.set_index('County2')[' Valoare'],on='County2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_counties['pop_density']=wiki_counties['Population (2011)[21]']/\\\n",
    "#     wiki_counties['Area[22]'].astype(str).str.split('km2').str[0].str.replace(',','').astype(int)\n",
    "wiki_counties['pop_density']=wiki_counties[' Valoare']/\\\n",
    "    wiki_counties['Area[22]'].astype(str).str.split('km2').str[0].str.replace(',','').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpdc.join(wiki_counties.set_index(['ISO code[note 3]'])).to_csv(htmlipath+'panels/case_density.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmsr['month']=dmsr.index.month\n",
    "dpdc2=dmsr.groupby(['month','iso']).max().reset_index().set_index('iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpdc2.join(wiki_counties.set_index(['ISO code[note 3]'])).to_csv(htmlipath+'panels/case_density2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kepler export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc=json.loads(open('../html/panels/ROU_adm1.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_dict={i['properties']['ID_1']:i for i in roc['features']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmsc=pd.concat(dmss)\n",
    "dmsc=dmsc[dmsc['lang']=='RO'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3017651"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('../html/panels/ROU_adm2.json','w').write(json.dumps(list(dmsc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355652"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rogeo=json.loads(open(htmlipath+'panels/ro.geo.json','r').read())\n",
    "for f in rogeo['features']:\n",
    "    f['properties']={'🌄':f['properties']['NAME_1']}\n",
    "open(htmlipath+'panels/ro.simple.geo.json','w').write(json.dumps(rogeo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locality map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagov1a=pd.read_excel('https://data.gov.ro/dataset/b86a78a3-7f88-4b53-a94f-015082592466/resource/bc19c354-644d-4a24-a26f-512129dbc70d/download/transparenta_vaccinare_martie_2021.xlsx')\n",
    "datagov2a=pd.read_excel('https://data.gov.ro/dataset/b86a78a3-7f88-4b53-a94f-015082592466/resource/d0b60b45-fb08-4980-a34c-8cbb4a43cad3/download/transparenta_martie_2021.xlsx',skiprows=1)\n",
    "datagov1b=pd.read_excel('https://data.gov.ro/dataset/b86a78a3-7f88-4b53-a94f-015082592466/resource/a0eb9ff2-9b97-430c-b285-360cadb55307/download/vaccinare-covid19-grupe-risc-01-01.04.2021.xlsx')\n",
    "datagov2b=pd.read_excel('https://data.gov.ro/dataset/b86a78a3-7f88-4b53-a94f-015082592466/resource/d3280256-07cc-4f93-957a-9815085899be/download/transparenta_aprilie_2021.xlsx',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagov1=pd.concat([datagov1a,datagov1b])\n",
    "datagov2=datagov2a.set_index(['Judet','UAT']).join(datagov2b.set_index(['Judet','UAT'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datagov1a\n",
    "del datagov1b\n",
    "del datagov2a\n",
    "del datagov2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "uat=json.loads(open(htmlipath+'panels/uat_simplificat.geojson','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs=[]\n",
    "for f in uat['features']:\n",
    "    locs.append(f['properties'])\n",
    "locs=pd.DataFrame(locs)[['judet','uat','siruta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judetconverter(c):\n",
    "    return (' '.join([i.capitalize() for i in c.split(' ')])).replace('ţ','ț').replace('ş','ș').replace('năsăud','Năsăud').\\\n",
    "        replace('severin','Severin').replace('mare','Mare').\\\n",
    "        replace('Municipiul București','Bucharest').replace('București','Bucharest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_iso=pd.DataFrame(iso_counties,index=['index']).T.reset_index().set_index('index').join(\\\n",
    "    pd.DataFrame(georoco,index=['geo_id']).T.reset_index().set_index('index')).reset_index()\n",
    "geo_iso.columns=['en','iso2','geo_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs['judet_norm']=[judetconverter(i) for i in locs['judet']]\n",
    "datagov2['judet_norm']=[judetconverter(i) for i in datagov2['Judet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uatconverter(judet,c):\n",
    "    c=(' '.join(['-'.join([b.capitalize() for b in a.split('-')]) for a in c.split(' ')]))\\\n",
    "        .replace('ţ','ț').replace('ş','ș').replace('Ţ','Ț').replace('Ş','Ș')\\\n",
    "        .replace('î','â').replace(' De ',' de ')\\\n",
    "        .replace('Cuza Voda','Cuza Vodă')\n",
    "    \n",
    "    if (judet=='Dolj'):\n",
    "        c=c.replace('Segarcea','Șegarcea')\n",
    "    elif (judet=='Teleorman'):\n",
    "        c=c.replace('Turnu Magurele','Turnu Măgurele')\n",
    "    elif (judet=='Olt'):\n",
    "        c=c.replace('Ipotesti','Ipotești')\n",
    "    elif (judet=='Alba'):\n",
    "        c=c.replace('Râmetea','Rimetea')\n",
    "    elif (judet=='Constanța'):\n",
    "        c=c.replace('44066','')\n",
    "    elif (judet=='Ialomița'):\n",
    "        c=c.replace('Radulești','Rădulești')\n",
    "    elif (judet=='Argeș'):\n",
    "        c=c.replace('Ciofringeni','Ciofrângeni')\\\n",
    "        .replace('Valea Mare Pravăț','Valea Mare-Pravăț')\n",
    "    elif (judet=='Vâlcea'):\n",
    "        c=c.replace('Păusești-Măglași','Păușești-Măglași')\n",
    "    elif (judet=='Prahova'):\n",
    "        c=c.replace('Cocorastii Colt','Cocorăștii Colț')\n",
    "    elif (judet=='Bucharest'):\n",
    "        c=c.replace('București Sectorul','Sector')\n",
    "    elif (judet=='Gorj'):\n",
    "        c=c.replace('Crușet','Crușeț')\n",
    "    elif (judet=='Brăila'):\n",
    "        c=c.replace('Racoviță','Racovița')\\\n",
    "        .replace('Gradiștea','Grădiștea')\\\n",
    "        .replace('Chișcani','Chiscani')\n",
    "    elif (judet=='Cluj'):\n",
    "        c=c.replace('Râșca','Rișca')\n",
    "    elif (judet=='Botoșani'):\n",
    "        c=c.replace('Răușeni','Răuseni')\n",
    "    elif (judet=='Maramureș'):\n",
    "        c=c.replace('Șisești','Șișești')\n",
    "    elif (judet=='Brașov'):\n",
    "        c=c.replace('Sambata de Sus','Sâmbăta de Sus')\n",
    "    elif (judet=='Vaslui'):\n",
    "        c=c.replace('Tătărăni','Tătărani')\n",
    "    elif (judet=='Iași'):\n",
    "        c=c.replace('Țigănăși','Țigănași')\n",
    "    elif (judet=='Bistrița-Năsăud'):\n",
    "        c=c.replace('Ilva Mica','Ilva Mică')\n",
    "    elif (judet=='Bihor'):\n",
    "        c=c.replace('Sănnicolau Romăn','Sânnicolau Român')\n",
    "    elif (judet=='Mureș'):\n",
    "        c=c.replace('Sarmașu','Sărmașu')\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagov2['uat_norm']=[(' '.join(['-'.join([b.capitalize() for b in a.split('-')]) for a in u.split(' ')]))\\\n",
    "                      .replace('ţ','ț').replace('ş','ș').replace('Ţ','Ț').replace('Ş','Ș')\\\n",
    "                      .replace(' De ',' de ').replace('Municipiul ','').replace('Oraș ','')\\\n",
    "                      for u in datagov2['UAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constanța 44066 \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "uat_norms=[]\n",
    "for judet in locs['judet_norm'].unique():\n",
    "    for l in locs[locs['judet_norm']==judet]['uat'].unique():\n",
    "        u=uatconverter(judet,str(l))\n",
    "        if u not in datagov2[datagov2['judet_norm']==judet]['uat_norm'].unique():\n",
    "            u=u.replace(' ','-')\n",
    "            if u not in datagov2[datagov2['judet_norm']==judet]['uat_norm'].unique():\n",
    "                print(judet,l,u)\n",
    "                u=''\n",
    "        uat_norms.append({'judet_norm':judet,'uat_norm':u,'uat':l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs2=locs.set_index(['judet_norm','uat']).join(pd.DataFrame(uat_norms).set_index(['judet_norm','uat'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagov3=datagov2.set_index(['judet_norm','uat_norm']).join(locs2.set_index(['judet_norm','uat_norm']))\n",
    "datagov3=datagov3.set_index(['judet','uat','siruta'],append=True).drop(['UAT','Judet'],axis=1)\n",
    "datagov3.index=datagov3.index.reorder_levels([2,3,4,0,1])\n",
    "# datagov3_dates=[pd.to_datetime(c) for c in datagov3.columns]\n",
    "# datagov3_dates=[c for c in datagov3.columns]\n",
    "datagov3_dates=[str(pd.to_datetime(c)) for c in datagov3.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagov3=datagov3.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datagov1\n",
    "del datagov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat(r):\n",
    "    if r<1: return '< 1 ‰'\n",
    "    elif r<2: return '1-2 ‰'\n",
    "    elif r<4: return '2-4 ‰'\n",
    "    elif r<6: return '4-6 ‰'\n",
    "    elif r<8: return '6-8 ‰'\n",
    "    elif r<10: return '8-10 ‰'\n",
    "    elif r<20: return '> 10-20 ‰'\n",
    "    else: return '> 20 ‰'\n",
    "def cat2(r):\n",
    "    if r<1: return 0\n",
    "    elif r<2: return 1\n",
    "    elif r<4: return 2\n",
    "    elif r<6: return 3\n",
    "    elif r<8: return 4\n",
    "    elif r<10: return 5\n",
    "    elif r<20: return 6\n",
    "    else: return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "new_uat={'type':'FeatureCollection','features':[]}\n",
    "new_uat_numbers=[]\n",
    "new_uat_daily={'type':'FeatureCollection','features':[]}\n",
    "for i in range(len(uat['features'])):\n",
    "    dummy={}\n",
    "    judet=uat['features'][i]['properties']['judet']\n",
    "    if (judet!=''):\n",
    "        u=uat['features'][i]['properties']['uat']\n",
    "        siruta=uat['features'][i]['properties']['siruta']\n",
    "        # dummy={'judet':judet,'uat':u,'siruta':siruta}\n",
    "        if u in datagov3.loc[judet].index:\n",
    "            d=datagov3.loc[judet].loc[u].loc[siruta]\n",
    "            values=d.values[0]\n",
    "            dummy['🌄']=d.index[0][0]\n",
    "            dummy['🏠']=d.index[0][1]\n",
    "            for t in range(len(values)):\n",
    "                value=values[t]\n",
    "                date=datagov3_dates[t]\n",
    "                dummy['📈']=value\n",
    "                dummy['🔴']=cat(value)\n",
    "                dummy['⚫']=cat2(value)\n",
    "                dummy['📆']=date\n",
    "                feature={'type':'Feature','geometry':uat['features'][i]['geometry'],'properties':dummy.copy()}\n",
    "                new_uat['features'].append(feature)\n",
    "                new_uat_numbers.append(dummy.copy())\n",
    "                if (t==len(datagov3_dates)-1):\n",
    "                    new_uat_daily['features'].append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255609508"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'panels/new_uat_numbers.json','w').write(json.dumps(new_uat_numbers))\n",
    "open(htmlipath+'panels/new_uat.json','w').write(json.dumps(new_uat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7100488"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kepler.gl export\n",
    "open(htmlipath+'panels/daily/new_uat'+date[:10]+'.json','w').write(json.dumps(new_uat_daily))\n",
    "open(htmlipath+'panels/new_uat_daily.json','w').write(json.dumps(new_uat_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_uat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapbox colors:\n",
    "# 239e69\n",
    "# f5d232\n",
    "# c52b69\n",
    "# 850237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D3plus export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagov4=datagov3.stack().reset_index()\n",
    "datagov4.columns=['judet','uat','id','judet_norm','uat_norm','date','value']\n",
    "datagov4['value']=np.round(datagov4['value'],2)\n",
    "# datagov4=datagov4.dropna(subset=['judet'])\n",
    "datagov4=datagov4.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17897760"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'panels/daily/uat2b_'+date[:10]+'.json','w').write(json.dumps(list(datagov4.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(new_uat_numbers)\n",
    "df4=df2[['🌄', '🏠', '📈','📆']].set_index(['🌄', '🏠', '📆']).unstack()['📈'].sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrictie(x,w):\n",
    "    if x>7.5: return 4\n",
    "    elif x>7:\n",
    "        if w>7.5: return 3\n",
    "        else: return 2\n",
    "    elif x>4: return 2\n",
    "    elif x>3.5:\n",
    "        if w>4: return 1\n",
    "        else: return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def restrictie2(x,w):\n",
    "    if x>7.5: return '7.5+'\n",
    "    elif x>7:\n",
    "        if w>7.5: return '7.5-'\n",
    "        else: return '4+'\n",
    "    elif x>4: return '4+'\n",
    "    elif x>3.5:\n",
    "        if w>4: return '4-'\n",
    "        else: return '0'\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr=[]\n",
    "for i in df4.T.iteritems():\n",
    "    v=i[1]\n",
    "    #print(i[0])\n",
    "    for j in range(len(v)):\n",
    "        t=v.index[j]\n",
    "        x=v[j]\n",
    "        y=v[j-min(14,j):j].values\n",
    "        w=x\n",
    "        if len(y):\n",
    "            w=max(y)\n",
    "        r=restrictie(x,w)\n",
    "        #print(j,t,x,w,r)\n",
    "        dr.append({'🌄':i[0][0], '🏠':i[0][1], '🚦':r,'📆':t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=pd.DataFrame(dr).set_index(['🌄', '🏠','📆']).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_uat_daily['features']:\n",
    "    i['properties']['🚦']=dp.loc[i['properties']['🌄']].loc[i['properties']['🏠']].loc[i['properties']['📆']]['🚦']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7167352"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'panels/new_uat_daily2.json','w').write(json.dumps(new_uat_daily))\n",
    "open(htmlipath+'panels/daily/new_uat_daily2_'+date[:10]+'.json','w').write(json.dumps(new_uat_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17323"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HTML overwrite\n",
    "uat_html=open(htmlipath+'panels/daily/mapbox-'+date[:10]+'.html','r').read()\n",
    "# uat_html=uat_html[:uat_html.find('.json')-2]+date[8:10]+uat_html[uat_html.find('.json'):]\n",
    "uat_html=uat_html[:uat_html.find('.json')-2]+\\\n",
    "    str(pd.to_datetime(now)+pd.to_timedelta('-1d'))[8:10]+\\\n",
    "    uat_html[uat_html.find('.json'):]\n",
    "uat_html=uat_html.replace(date[5:10],date[5:8]+today0).replace(date[5:10],date[5:8]+today0)\n",
    "open(htmlipath+'panels/daily/mapbox-'+date[:8]+today0+'.html','w').write(uat_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to `uat.json` to mapbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobility map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=mobility[mobility['lang']=='HU']\n",
    "dm=dm[dm['langtype']!=szotar['ro']['HU']]\n",
    "\n",
    "# dn=dm.reset_index().set_index(['date','langtype','indicator'])[['value']].unstack()['value'].reset_index().set_index('date')\n",
    "dm=dm.reset_index().set_index(['date','langtype','indicator'])[['value']]\n",
    "dm=dm[~dm.index.duplicated(keep='first')]\n",
    "dn=dm.unstack()['value'].reset_index().set_index('date')\n",
    "\n",
    "dn['residential_percent_change_from_baseline']=-dn['residential_percent_change_from_baseline']\n",
    "dw=dn.reset_index().set_index(['date']).sort_index()['2020-02-17':].groupby('langtype')\\\n",
    "    .resample('7D',label='right').mean().reset_index()\n",
    "dw['date']-=pd.to_timedelta('1d')\n",
    "dw['date']=dw['date'].dt.strftime('%Y-%b-%d').astype('str')#.str[:8]\n",
    "dw=pd.DataFrame(np.round(dw.set_index(['date','langtype']).T.mean(),0)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del mobility #free up memory\n",
    "# del dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw=dw.join(dh.set_index('HU'),on='langtype').drop(1,axis=1)\n",
    "dw.columns=['date','HU','mobility','RO','EN','county']\n",
    "dw=dw.reset_index().dropna()\n",
    "for lang in languages:\n",
    "    dc=dw[['date',lang,'mobility','county']]\n",
    "    dc.columns=['date','county','value','id']\n",
    "    for c in ['value']:\n",
    "        dd=dc.set_index(['date',c])[['county']].sort_index(ascending=False)\n",
    "        dds=[]\n",
    "        for d in dd.index.get_level_values(0).unique():\n",
    "            dds.append(dd.loc[[d]].iloc[:top])\n",
    "            dds.append(dd.loc[[d]].iloc[-top:])\n",
    "        dds=pd.concat(dds)\n",
    "        dds[c+'_display']=True\n",
    "        dc=dc.set_index(['date','county']).join(dds.reset_index().set_index(['date','county'])[c+'_display']).reset_index()\n",
    "        dc[c+'_display']=dc[c+'_display'].fillna(False)\n",
    "        dc=dc.drop_duplicates()\n",
    "        dc=dc.set_index(['county','date']).unstack().dropna(how='any',axis=1).stack().reset_index()\n",
    "#         dc=dc.set_index(pd.to_datetime(dc['date'])).sort_index().loc['2020-12-01':].drop('date',axis=1).reset_index()\n",
    "        dc=dc.set_index(pd.to_datetime(dc['date'])).sort_index().drop('date',axis=1).reset_index()\n",
    "        dc['date']=dc['date'].astype(str).str[:10]\n",
    "        dc=dc.dropna()\n",
    "#         dc=dc.set_index(pd.to_datetime(dc['date'])).sort_index().loc['2020-12-01':].reset_index(drop=True)\n",
    "        \n",
    "#         dc=dc[pd.to_datetime(dc['date'])<'2020-08-10'] #DIASBLE LATER WHEN GOOGLE MOBILITY BACK TO NORMAL!\n",
    "    open(htmlipath+'panels/county3ag_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))\n",
    "    # open('county3_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobility counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw=dn.reset_index().set_index(['date']).sort_index()['2020-02-16':].groupby('langtype')\\\n",
    "    .resample('1D',label='left').mean().reset_index()\n",
    "dw=pd.DataFrame(np.round(dw.set_index(['date','langtype']).T.mean(),0)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx=dw.join(dh.set_index('HU'),on='langtype').drop([1,'county'],axis=1)\n",
    "dx.columns=['date','HU','value','RO','EN']\n",
    "dx=pd.DataFrame(dx.set_index(['date','value']).stack().reset_index()).set_index('date')\n",
    "dx.columns=['value','lang','langtype']\n",
    "dx['value']=np.round(dx['value']*1.0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs=[]\n",
    "for lang in languages:\n",
    "    dr=mobility_mini.copy()#*100\n",
    "    dr['lang']=lang\n",
    "    dr['langtype']=szotar['ro'][lang]\n",
    "    drs.append(dr)\n",
    "drs=pd.concat(drs)\n",
    "drs.columns=['value','lang','langtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing mobility4 from 2021-03-24 00:00:00+00:00 ...\n",
      "Writing to mobility4 ...\n",
      "1029 data points will be written in 0.2058 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langtype']\n",
    "measurement='mobility4'\n",
    "push2influx(pd.concat([dx,drs],axis=0).dropna(),measurement,field_columns,tag_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imobiliare map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='realEstateTransactions'\n",
    "di=pd.read_csv(url+sheet, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "di[1]=di[1].replace({'BISTRIȚA NĂSĂUD':'BISTRIȚA-NĂSĂUD','BACAU':'BACĂU',\n",
    "                       'CARAȘ SEVERIN':'CARAȘ-SEVERIN','DAMBOVIȚA':'DÂMBOVIȚA','IASI':'IAȘI'})\n",
    "di=di.dropna(axis=1,how='all').set_index(1)[[2,5,8,11,14,17,20,23,26,29,32,35,38,41]][1:-1].dropna()\n",
    "di.columns=['2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01','2020-06-01','2020-07-01',\n",
    "            '2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01']\n",
    "for c in di.columns:\n",
    "    di[c]=di[c].str.replace(',','').astype(float)\n",
    "dj=di.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "djm=(di['2020-01-01']+di['2020-02-01']+di['2020-03-01'])/3\n",
    "for c in dj.columns:\n",
    "    dj[c]=-100+(di[c]*100/djm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "djro=pd.DataFrame(dj.mean())\n",
    "djro.columns=['ROMÂNIA']\n",
    "dj=pd.concat([dj,djro.T])\n",
    "\n",
    "diro=pd.DataFrame(di.sum())\n",
    "diro.columns=['ROMÂNIA']\n",
    "di=pd.concat([di,diro.T])\n",
    "\n",
    "dhr=pd.concat([dh,pd.DataFrame(szotardf.loc['ro'][languages]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "di=di.join(dhr.set_index(dhr['RO'].str.upper())).set_index(languages+['county'])[di.columns].stack().reset_index()\n",
    "di.columns=languages+['county','date','value']\n",
    "di['date']=(pd.to_datetime(di['date'])-pd.to_timedelta('1d')).astype(str).str[:10]\n",
    "for lang in languages:\n",
    "    dc=di.reset_index()[['date',lang,'value','county']].dropna()\n",
    "    dc.columns=['date','county','value','id']\n",
    "    open(htmlipath+'panels/county4_202101'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj=dj.join(dhr.set_index(dhr['RO'].str.upper())).set_index(languages+['county'])[dj.columns].stack().reset_index()\n",
    "dj.columns=languages+['county','date','value']\n",
    "dj['value']=np.round(dj['value'],0)\n",
    "dj['date']=(pd.to_datetime(dj['date'])-pd.to_timedelta('1d')).astype(str).str[:10]\n",
    "for lang in languages:\n",
    "    dc=dj.reset_index()[['date',lang,'value','county']].dropna()\n",
    "    dc.columns=['date','county','value','id']\n",
    "    for c in ['value']:\n",
    "        dd=dc.set_index(['date',c])[['county']].sort_index(ascending=False)\n",
    "        dds=[]\n",
    "        for d in dd.index.get_level_values(0).unique():\n",
    "            dds.append(dd.loc[[d]].iloc[:top])\n",
    "            dds.append(dd.loc[[d]].iloc[-top:])\n",
    "        dds=pd.concat(dds)\n",
    "        dds[c+'_display']=True\n",
    "        dc=dc.set_index(['date','county']).join(dds.reset_index().set_index(['date','county'])[c+'_display']).reset_index()\n",
    "        dc[c+'_display']=dc[c+'_display'].fillna(False)\n",
    "    open(htmlipath+'panels/county4b_202101'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imobiliare counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "di=di.set_index(['county','date','value']).stack().reset_index().set_index('date')\n",
    "di.columns=['county','value','lang','langtype']\n",
    "di.index=pd.to_datetime(di.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging imobiliare1 ...\n",
      "Writing to imobiliare1 ...\n",
      "1806 data points will be written in 0.3612 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langtype']\n",
    "measurement='imobiliare1'\n",
    "push2influx(di,measurement,field_columns,tag_columns,fo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj=dj.set_index(['county','date','value']).stack().reset_index().set_index('date')\n",
    "dj.columns=['county','value','lang','langtype']\n",
    "dj.index=pd.to_datetime(dj.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging imobiliare2 ...\n",
      "Writing to imobiliare2 ...\n",
      "1806 data points will be written in 0.3612 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['value']\n",
    "tag_columns=['lang','langtype']\n",
    "measurement='imobiliare2'\n",
    "push2influx(dj,measurement,field_columns,tag_columns,fo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imobiliare cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='realEstatePrices'\n",
    "dp0=pd.read_csv(url+sheet, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=dp0.dropna(axis=1)[1:7]\n",
    "dp.columns=['county','2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01',\n",
    "            '2020-06-01','2020-07-01','2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01']\n",
    "dp2=dp0.dropna(axis=1)[8:]\n",
    "dp2.columns=['county','2008-07-01','2008-08-01','2008-09-01','2008-10-01','2008-11-01',\n",
    "             '2008-12-01','2009-01-01','2009-02-01','2009-03-01','2009-04-01','2009-05-01','2009-06-01','2009-07-01','2009-08-01']\n",
    "# dp2.columns=dp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=dp.set_index('county').T.stack().reset_index()\n",
    "dp.columns=['date','county','value']\n",
    "dp['date']=pd.to_datetime(dp['date'])-pd.to_timedelta('1d')\n",
    "dp['date']=pd.to_datetime(dp['date'])\n",
    "dp=dp.set_index('date')\n",
    "dp['value']=dp['value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp2=dp2.set_index('county')\n",
    "dp2.columns=pd.to_datetime(dp2.columns)-pd.to_timedelta('1d')\n",
    "dp2=dp2.T\n",
    "dp2['Year']=2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=dp.reset_index().set_index(['date','county']).unstack()['value']\n",
    "dp['Year']=2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp=pd.concat([dp,dp2])\n",
    "dpp.index=dpp.index.astype(str).str[:10]\n",
    "dpp.to_excel(htmlipath+'panels/imobiliare1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp=dpp.set_index('Year',append=True).stack().reset_index()\n",
    "dpp.columns=['date','year','city','price']\n",
    "dpp['price']=dpp['price'].astype(float)\n",
    "dpp['date']=pd.to_datetime(dpp['date'])\n",
    "dpp=dpp.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp=dpp.reset_index().set_index('city').join(szotardf[languages]).set_index(['date','year','price']).stack().reset_index()\n",
    "dpp.columns=['date','year','price','lang','city']\n",
    "dpp['cityyear']=dpp['city']+' '+dpp['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp=dpp.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging imobiliare3 ...\n",
      "Writing to imobiliare3 ...\n",
      "504 data points will be written in 0.1008 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['price']\n",
    "tag_columns=['city','year','lang','cityyear']\n",
    "measurement='imobiliare3'\n",
    "push2influx(dpp,measurement,field_columns,tag_columns,dbclient=client_long,fo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imobiliare cities map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords={'Brasov':'45.64861, 25.60613', \n",
    "        'Bucharest':'44.43225, 26.10626', \n",
    "        'Cluj':'46.76667, 23.6', \n",
    "        'Constanta':'44.18073, 28.63432', \n",
    "        'Timisoara':'45.75372, 21.22571',\n",
    "        'Max':'0,0'}\n",
    "popo={'Brasov':276088, \n",
    "        'Bucharest':1877155, \n",
    "        'Cluj':316748, \n",
    "        'Constanta':303399, \n",
    "        'Timisoara':315053,\n",
    "        'Max':2000000}\n",
    "pop={'Brasov':276088, \n",
    "        'Bucharest':500000, \n",
    "        'Cluj':316748, \n",
    "        'Constanta':303399, \n",
    "        'Timisoara':315053,\n",
    "        'Max':500000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp2=dp2[:1].T\n",
    "dp2.columns=['2019-11-30']\n",
    "dp=dp.T.join(dp2).T\n",
    "dp['Max']=2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "do=dp.T.join(pd.DataFrame(coords,index=['coords']).T).dropna().join(pd.DataFrame(pop,index=['pop']).T).\\\n",
    "    set_index(['coords','pop'],append=True).stack().reset_index()\n",
    "do['lat']=do['coords'].str.split(',').str[0].str.strip().astype(float)\n",
    "do['lon']=do['coords'].str.split(',').str[1].str.strip().astype(float)\n",
    "do.columns=['county','coords','pop','date','price','lat','lon']\n",
    "do['date']=do['date'].astype('str').str[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    dc=do.copy()\n",
    "    dc['county']=dc['county'].replace(szotardf[lang].to_dict())\n",
    "    open(htmlipath+'panels/county4c_202101'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='matrix'\n",
    "df=pd.read_csv(url+sheet)[:-1]\n",
    "df=df[['IND HU short','IND RO short','IND EN short','% of GDP (2018)',\n",
    "       'GDP gain/loss in 2020 Q2 (%)','GDP gain/loss in 2020 Q3 (%)']]\n",
    "df.columns=['HU','RO','EN','share','q2','q3']\n",
    "df['share']=df['share'].str.replace('%','').astype(float)\n",
    "for q in ['q2','q3']:\n",
    "    df[q]=df[q].str.replace('%','').astype(float)\n",
    "# df=pd.DataFrame(df.set_index(languages+['share']).stack()).reset_index()\n",
    "# df.columns=languages+['share','quarter','value']\n",
    "# df['group']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-873-86558fe078b8>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dc['c2']=(dc['x']/dc['x'].median())*dc['q2']\n",
      "<ipython-input-873-86558fe078b8>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dc['c2']=(dc['x']/dc['x'].median())*dc['q2']\n",
      "<ipython-input-873-86558fe078b8>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dc['c2']=(dc['x']/dc['x'].median())*dc['q2']\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    dc=df[[lang,'share','q2','q3']]\n",
    "    dc.columns=['id','x','q2','q3']\n",
    "    dc['c2']=(dc['x']/dc['x'].median())*dc['q2']\n",
    "    dc['c3']=(dc['x']/dc['x'].median())*dc['q3']\n",
    "    open(htmlipath+'panels/matrix_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))\n",
    "    # open('matrix_'+lang+'.json','w').write(json.dumps(list(dc.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=szotardf[languages].loc[['% of GDP (2018)','GDP gain/loss in 2020 Q2 (%)']].T\n",
    "ds.columns=['x','y']\n",
    "open(htmlipath+'panels/matrix_label.json','w').write(json.dumps(ds.to_dict()))\n",
    "#open('matrix_label.json','w').write(json.dumps(ds.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Severity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Jake VanderPlas\n",
    "# License: BSD-style\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    #    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import to_hex\n",
    "cm = LinearSegmentedColormap.from_list(\n",
    "        'my_cmap', ['gold','orange','crimson','darkRed'] , N=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity['month']=severity.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1444"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_label=szotardf.loc[['severity_plot2x','severity_plot2y','severity_plot2s',\n",
    "                      'severity_q1','severity_q2','severity_q3','severity_q4'\n",
    "                      ]][languages]\n",
    "sv_label.index=['x','y','s','q1','q2','q3','q4']\n",
    "open(htmlipath+'panels/severity_label2.json','w').write(json.dumps(sv_label.T.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU\n",
      "RO\n",
      "EN\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    smo=['2020-02-28','2020-03-31','2020-04-30','2020-05-31','2020-06-30','2020-07-31','2020-08-31',\n",
    "         '2020-09-30','2020-10-31','2020-11-30','2020-12-31','2021-01-31','2021-02-28',str(severity.index.max())[:10]]\n",
    "    sv=df0.loc[smo]['cases'].astype(int)\n",
    "    svd=sv.diff()\n",
    "    svd.loc['2020-02-28']=sv.loc['2020-02-28']\n",
    "    svd.name=sv_label.loc['s'][lang]\n",
    "\n",
    "    severity['month']=severity.index.strftime('%Y-%m')\n",
    "    sy=severity[((severity['type']=='stringency')&(severity['lang']==lang)&(severity['lang2']==lang))]\\\n",
    "        .drop(['country','langcountry','lang','lang2','type','langtype'],axis=1).groupby('month').mean()\n",
    "    sy.index=smo\n",
    "    sy=sy['value']\n",
    "    sy.name=sv_label.loc['y'][lang]\n",
    "    sy.index=pd.to_datetime(sy.index)\n",
    "\n",
    "    svy=pd.DataFrame(svd).join(sy)\n",
    "    svy.index=pd.to_datetime(svy.index)\n",
    "    svy['month']=svy.index.strftime('%Y-%m')\n",
    "\n",
    "    sm=pd.concat([dx,drs],axis=0)\n",
    "    sm=sm[sm['langtype']==szotar['ro'][lang]]\n",
    "    sm['month']=sm.index.strftime('%Y-%m')\n",
    "    sm=-sm.groupby('month').mean()['value']\n",
    "    sm.name=sv_label.loc['x'][lang]\n",
    "    svy=svy.set_index('month').join(sm).reset_index()\n",
    "    svy['color']=[to_hex(cm(i/max(svy[sv_label.loc['s'][lang]]))) for i in svy[sv_label.loc['s'][lang]]]\n",
    "    #     svy=pd.concat([svy,pd.DataFrame([{'month':0,sv_label.loc['x'][lang]:20,sv_label.loc['y'][lang]:20,sv_label.loc['s'][lang]:1,'color':'#000000'}])])\n",
    "    #     svy.loc[0,sv_label.loc['s'][lang]]=1000\n",
    "    #     svy=svy.dropna() #also implies deleting A9 on the list below\n",
    "    svy=svy.ffill()\n",
    "    svy1=svy[((pd.to_datetime(svy['month'])>=pd.to_datetime('2020-04'))&((pd.to_datetime(svy['month'])<=pd.to_datetime('2020-08'))))]\\\n",
    "        .sort_values(by='month',ascending=False)[[sv_label.loc['x'][lang],sv_label.loc['y'][lang]]]\n",
    "    svy1=pd.concat([pd.DataFrame({sv_label.loc['x'][lang]: 42, sv_label.loc['y'][lang]: 89},index=[98]),\n",
    "                    pd.DataFrame({sv_label.loc['x'][lang]: 25, sv_label.loc['y'][lang]: 64},index=[96]),\n",
    "                    pd.DataFrame({sv_label.loc['x'][lang]: 6, sv_label.loc['y'][lang]: 45},index=[94]),\n",
    "                    pd.DataFrame({sv_label.loc['x'][lang]: 0, sv_label.loc['y'][lang]: 39},index=[93]),\n",
    "                    pd.DataFrame({sv_label.loc['x'][lang]: 16, sv_label.loc['y'][lang]: 51},index=[95]),svy1])\n",
    "    svy1['line']='A'\n",
    "    svy2=svy[((pd.to_datetime(svy['month'])>=pd.to_datetime('2020-02'))&((pd.to_datetime(svy['month'])<=pd.to_datetime('2020-04'))))]\\\n",
    "        .sort_values(by='month',ascending=True)[[sv_label.loc['x'][lang],sv_label.loc['y'][lang]]]\n",
    "    svy2=pd.concat([pd.DataFrame({sv_label.loc['x'][lang]: 41, sv_label.loc['y'][lang]: 75},index=[99]),svy2])\n",
    "    svy2['line']='B'\n",
    "    svy3=svy[((pd.to_datetime(svy['month'])>=pd.to_datetime('2020-08'))&((pd.to_datetime(svy['month'])<pd.to_datetime('2021-02'))))]\\\n",
    "        .sort_values(by='month',ascending=True)[[sv_label.loc['x'][lang],sv_label.loc['y'][lang]]]\n",
    "    svy3=pd.concat([pd.DataFrame({sv_label.loc['x'][lang]: 19, sv_label.loc['y'][lang]: 61},index=[97]),svy3])\n",
    "    svy3['line']='C'\n",
    "    open(htmlipath+'panels/severity1_4_'+lang+'.json','w').write(json.dumps(\n",
    "        {'circles':list(np.round(svy,1).reset_index().T.to_dict().values()),\n",
    "        'lines1b':list(svy1.T.to_dict().values()),\n",
    "        'lines2b':list(svy2.T.to_dict().values()),\n",
    "        'lines3b':list(svy3.T.to_dict().values())\n",
    "        }))\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart `InfluxDB` and `Grafana` to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "icons={'YoY%':'📈', '%':'%', 'proxy index':'➰', 'value index':'💲',\n",
    "       'volume index':'🛒', 'index':'〽'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-596-77652dbc15c9>:12: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  szotarHU=df.set_index('HU',drop=False).T.to_dict()\n",
      "<ipython-input-596-77652dbc15c9>:13: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  szotarRO=df.set_index('RO',drop=False).T.to_dict()\n",
      "<ipython-input-596-77652dbc15c9>:14: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  szotarEN=df.set_index('EN',drop=False).T.to_dict()\n"
     ]
    }
   ],
   "source": [
    "szotardf,szotar,szotarHU,szotarRO,szotarEN=get_szotar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet='EcMonitor'\n",
    "df=pd.read_csv(url+sheet,header=None).dropna(how='all',axis=1).set_index(0)\n",
    "df.index.name='indicator'\n",
    "df.loc['GDP']=df.loc['GDP'].bfill()\n",
    "df.loc['Market consensus']=df.loc['Market consensus'].bfill()\n",
    "df['order']=[100*i for i in range(len(df.index))]\n",
    "df=df.set_index('order',append=True)\n",
    "df=df.drop(['Indicator'],axis=0,level=0)\n",
    "df2=df[1:].drop([1],axis=1)\n",
    "# df2=df2.dropna(how='all')\n",
    "sections=[i[0] for i in df[df.isnull().all(axis=1)].index]\n",
    "df[1]=df[1].fillna('row')\n",
    "# df=df[df[1:].dropna(how='all',axis=1).columns]\n",
    "df[1]=df[1].replace(icons)\n",
    "df1=df.stack().dropna().reset_index()\n",
    "df1.columns=['indicator','order','index','value']\n",
    "df2=df2.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "cscaler=1\n",
    "colors=df2.reset_index().set_index('indicator').T.count().to_dict()\n",
    "colors={i:{'size':colors[i]} for i in colors}\n",
    "for i in df2.index:\n",
    "    colors[i[0]]['min']=float(df2.loc[i[0]].min().min()*cscaler)\n",
    "    colors[i[0]]['max']=float(df2.loc[i[0]].max().max()*cscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist=[]\n",
    "for i in df1[['indicator','value','index']].T.iteritems():\n",
    "    if i[1][2]==2:\n",
    "        if i[1][0] not in ['date']:\n",
    "            ref_color=float(i[1][1])\n",
    "            if i[1][0] in ['GDP','Market consensus','DFM estimate','No. of employees']:\n",
    "                ref_color=0\n",
    "            if i[1][0] in ['Construction works']:\n",
    "                ref_color=20\n",
    "            if i[1][0] in ['Consumer price index']:\n",
    "                ref_color=3\n",
    "            if i[1][0] in ['Non-government loans']:\n",
    "                ref_color=6\n",
    "            if i[1][0] in ['Non-government deposits']:\n",
    "                ref_color=10.7\n",
    "            color='transparent'\n",
    "    elif i[1][2]==1:\n",
    "        color='transparent'\n",
    "    elif i[1][0] in ['date']:\n",
    "        color='transparent'\n",
    "    elif i[1][0] not in colors:\n",
    "        color='transparent'\n",
    "    else:\n",
    "        #if i[1][0] in ['GDP','Market consensus','% of information included in the model']:\n",
    "        if i[1][0] in ['% of information included in the model']:\n",
    "            cmap='Blues'\n",
    "        else:\n",
    "            cmap='RdBu'\n",
    "        rc=(ref_color*1.0-colors[i[1][0]]['min'])*1.0/(colors[i[1][0]]['max']*1.0-colors[i[1][0]]['min']*1.0)\n",
    "        c=(float(i[1][1])*1.0-colors[i[1][0]]['min'])*1.0/(colors[i[1][0]]['max']*1.0-colors[i[1][0]]['min']*1.0)\n",
    "        if rc==0: rc=0.1\n",
    "        c=c*0.5/rc\n",
    "        if (i[1][0] in ['Unemployment ratio']):\n",
    "            c=1-c\n",
    "        # color=adjust_lightness(discrete_cmap(colors[i[1][0]]['size'],cmap)(c),1)\n",
    "        color=adjust_lightness(discrete_cmap(20,cmap)(c),1)\n",
    "    colorlist.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuelist=[]\n",
    "for i in df1[['indicator','value','index']].T.iteritems():\n",
    "    if i[1][2]==1:\n",
    "        valuelist.append(i[1][1])\n",
    "    elif i[1][0] in colors:\n",
    "        if float(i[1][1])<100:\n",
    "            valuelist.append(str(np.round(float(i[1][1]),1)).replace('.0',''))\n",
    "        else:\n",
    "            valuelist.append(str(int(np.round(float(i[1][1]),0))))\n",
    "    else:\n",
    "        valuelist.append(i[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['color']=colorlist\n",
    "df1['value']=valuelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets={'groups':[],'lang':szotardf.loc[list(df.droplevel('order')[2].index)+['data','desc','type','month']][:].T.to_dict()}\n",
    "sets['lang']={i:{k:sets['lang'][i][k] for k in sets['lang'][i] if str(sets['lang'][i][k]) not in ['nan','XXX']} for i in sets['lang']}\n",
    "for i in df.droplevel('order')[1:][2].iteritems():\n",
    "    if str(i[1])=='nan':\n",
    "        #sets['groups'].append([])\n",
    "        sets['groups'].append([i[0]])\n",
    "    else:\n",
    "        sets['groups'][-1].append(i[0])\n",
    "# sets[0]=[item for sublist in sets['groups'] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_months={'2015-2019':{'HU':'2015-2019','RO':'2015-2019','EN':'2015-2019'},\n",
    "            '2019-06':{'HU':'2019 Jún','RO':'2019 Iun','EN':'2019 Jun'},\n",
    "             '2019-07':{'HU':'Júl','RO':'Iul','EN':'Jul'},\n",
    "            '2019-08':{'HU':'Aug','RO':'Aug','EN':'Aug'},\n",
    "             '2019-09':{'HU':'2019 Sze','RO':'2019 Sep','EN':'2019 Sep'},\n",
    "             '2019-10':{'HU':'Okt','RO':'Oct','EN':'Oct'},\n",
    "             '2019-11':{'HU':'Nov','RO':'Noi','EN':'Nov'},\n",
    "             '2019-12':{'HU':'Dec','RO':'Dec','EN':'Dec'},\n",
    "             '2020-01':{'HU':'2020 Jan','RO':'2020 Ian','EN':'2020 Jan'},\n",
    "             '2020-02':{'HU':'Feb','RO':'Feb','EN':'Feb'},\n",
    "             '2020-03':{'HU':'Már','RO':'Mar','EN':'Mar'},\n",
    "             '2020-04':{'HU':'Ápr','RO':'Apr','EN':'Apr'},\n",
    "             '2020-05':{'HU':'Máj','RO':'Mai','EN':'May'},\n",
    "             '2020-06':{'HU':'Jún','RO':'Iun','EN':'Jun'},\n",
    "             '2020-07':{'HU':'Júl','RO':'Iul','EN':'Jul'},\n",
    "             '2020-08':{'HU':'Aug','RO':'Aug','EN':'Aug'},\n",
    "             '2020-09':{'HU':'Sze','RO':'Sep','EN':'Sep'},\n",
    "            '2020-10':{'HU':'Okt','RO':'Oct','EN':'Oct'},\n",
    "            '2020-11':{'HU':'Nov','RO':'Noi','EN':'Nov'},\n",
    "            '2020-12':{'HU':'Dec','RO':'Dec','EN':'Dec'},\n",
    "            '2021-01':{'HU':'2021 Jan','RO':'2021 Ian','EN':'2021 Jan'},\n",
    "             '2021-02':{'HU':'Feb','RO':'Feb','EN':'Feb'},\n",
    "             '2021-03':{'HU':'Már','RO':'Mar','EN':'Mar'},\n",
    "             }\n",
    "sets_months2={\n",
    "            '2019-06':{'HU':'Június','RO':'Iunie','EN':'June'},\n",
    "             '2019-07':{'HU':'Július','RO':'Iulie','EN':'July'},\n",
    "            '2019-08':{'HU':'Augusztus','RO':'August','EN':'August'},\n",
    "             '2019-09':{'HU':'Szeptember','RO':'Septembrie','EN':'September'},\n",
    "             '2019-10':{'HU':'Október','RO':'Octombrie','EN':'October'},\n",
    "             '2019-11':{'HU':'November','RO':'Noiembrie','EN':'November'},\n",
    "             '2019-12':{'HU':'December','RO':'Decembrie','EN':'December'},\n",
    "             '2020-01':{'HU':'Január','RO':'Ianuarie','EN':'January'},\n",
    "             '2020-02':{'HU':'Február','RO':'Februarie','EN':'February'},\n",
    "             '2020-03':{'HU':'Március','RO':'Martie','EN':'March'},\n",
    "             '2021-01':{'HU':'Január','RO':'Ianuarie','EN':'January'},\n",
    "             '2021-02':{'HU':'Február','RO':'Februarie','EN':'February'},\n",
    "             '2021-03':{'HU':'Március','RO':'Martie','EN':'March'},\n",
    "             '2020-04':{'HU':'Április','RO':'Aprilie','EN':'April'},\n",
    "             '2020-05':{'HU':'Május','RO':'Mai','EN':'May'},\n",
    "             '2020-06':{'HU':'Június','RO':'Iunie','EN':'June'},\n",
    "             '2020-07':{'HU':'Július','RO':'Iulie','EN':'July'},\n",
    "             '2020-08':{'HU':'Augusztus','RO':'August','EN':'August'},\n",
    "             '2020-09':{'HU':'Szeptember','RO':'Septembrie','EN':'September'},\n",
    "            '2020-10':{'HU':'Október','RO':'Octombrie','EN':'October'},\n",
    "            '2020-11':{'HU':'November','RO':'Noiembrie','EN':'November'},\n",
    "            '2020-12':{'HU':'December','RO':'Decembrie','EN':'December'}}\n",
    "sets_months['type']=szotardf.loc['type'][languages].to_dict()\n",
    "sets['months']={i:sets_months[i] for i in df.loc['date'].values[0]}\n",
    "sets_labels=df[:1].T['date'].to_dict()[0]\n",
    "sets_labels={i:{j:sets_labels[i].split('-')[0]+' '+sets_months2[sets_labels[i]][j] \\\n",
    "    for j in sets_months2[sets_labels[i]]} for i in sets_labels if sets_labels[i] in sets_months2}\n",
    "sets_labels['indicator']=szotardf.loc['indicator'][languages].to_dict()\n",
    "sets_labels[2]={'HU':'2015-2019 átlag','RO':'medie 2015-2019','EN':'2015-2019 average'}\n",
    "sets['label']=sets_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20825"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'panels/macro_sets76.json','w').write(json.dumps(sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106895"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df1.join(pd.DataFrame(sets['label']).T,on='index')\n",
    "df4=df3[['value']+languages].T.ffill()\n",
    "for lang in languages:\n",
    "    df3[lang]=df4.T[lang].replace({icons[i]:szotar[i][lang] for i in icons})\n",
    "open(htmlipath+'panels/macro_table77.json','w').write(json.dumps(list(df3.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_macro=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_long.drop_database(dbname_long)\n",
    "# client_long.drop_retention_policy(dbname_long)\n",
    "# client_long.create_database(dbname_long)\n",
    "# client_long.create_retention_policy(dbname_long, '6000d', 1, default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_macro:\n",
    "    sheet='DFMChartYoY'\n",
    "    df=pd.read_csv(url+sheet)\n",
    "    df=df.set_index('Date').loc[['DFM estimate','GDP','GDP.1','Market consensus','Range68']].T.\\\n",
    "        dropna(how='all',axis=0).astype(float).reset_index()\n",
    "    df.index=[(pd.to_datetime('now')-pd.to_timedelta('10H')*(1+i)).strftime('%Y-%m-%d-%H') for i in range(len(df))][::-1]\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "    df['index']=pd.to_datetime(df['index'].str.replace('-','-20'))\n",
    "    df['CI+']=df['DFM estimate']+df['Range68']/2\n",
    "    df['CI-']=df['DFM estimate']-df['Range68']/2\n",
    "    df['CI++']=df['DFM estimate']+df['Range68']\n",
    "    df['CI--']=df['DFM estimate']-df['Range68']\n",
    "    df=pd.DataFrame(df.set_index('index').stack()).reset_index()\n",
    "    df.columns=['date','type','value']\n",
    "    df=df.set_index('date')\n",
    "    df['value']=df['value'].astype(float)\n",
    "    df=df.join(szotardf,on='type')\n",
    "    df=pd.DataFrame(df.reset_index().set_index(['date','type','value']).stack()).reset_index().set_index('date')\n",
    "    df.columns=['type','value','lang','langtype']\n",
    "    df.index=pd.to_datetime(df.index)+pd.DateOffset(months=1)-pd.DateOffset(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging macro ...\n",
      "Writing to macro ...\n",
      "5652 data points will be written in 1.1304 batches.\n",
      "Expected query running time is: 5 seconds.\n",
      "Writing batch 1 ...\n",
      "Writing batch 2 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if process_macro:\n",
    "    field_columns=['value']\n",
    "    tag_columns=['lang','type','langtype']\n",
    "    measurement='macro'\n",
    "    push2influx(df,measurement,field_columns,tag_columns,dbclient=client_long,fo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate data complexity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements=client.query('show measurements')\n",
    "measurements_long=client_long.query('show measurements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "counties\n",
      "counties2\n",
      "counties3\n",
      "firms1\n",
      "firms2\n",
      "firms3\n",
      "firms4\n",
      "forecast\n",
      "global2\n",
      "global3\n",
      "global4\n",
      "governance1\n",
      "governance2\n",
      "governance3\n",
      "imobiliare1\n",
      "imobiliare2\n",
      "mobility\n",
      "mobility2\n",
      "mobility3\n",
      "mobility4\n",
      "news1\n",
      "registry1\n",
      "severity\n",
      "social1\n",
      "social2\n",
      "social3\n",
      "stocks1\n",
      "stocks2\n",
      "vaccine1\n",
      "imobiliare3\n",
      "macro\n"
     ]
    }
   ],
   "source": [
    "ms={}\n",
    "for m in [j['name'] for j in [i for i in measurements][0]]:\n",
    "    print(m)\n",
    "    time.sleep(1)\n",
    "    ms[m]=client.query('SELECT COUNT(*) FROM '+m)\n",
    "for m in [j['name'] for j in [i for i in measurements_long][0]]:\n",
    "    print(m)\n",
    "    time.sleep(1)\n",
    "    ms[m]=client_long.query('SELECT COUNT(*) FROM '+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(languages)\n",
    "cat={'status':['governance1','governance2','counties','counties2','counties3']+['governance3']*3,\n",
    "    'global':['global3','global4']+['global2']*l,\n",
    "    'governance':['social1','social2','social3']*l+['severity'],\n",
    "    'news':['news1']*l*2,\n",
    "    'macro':['macro'],\n",
    "    'stocks':['stocks1','stocks2'],\n",
    "    'firms':['firms1','firms4']+['firms2','firms3']*l,\n",
    "    'industry':[],\n",
    "    'mobility':['mobility','mobility2','mobility3','mobility4'],\n",
    "    'imobiliare':['imobiliare1','imobiliare2','imobiliare3']}\n",
    "mg={i:0 for i in cat}\n",
    "ms2={}\n",
    "for m in ms:\n",
    "    for d in ms[m]:\n",
    "        ms2[d]=ms[m][d].sum().sum()\n",
    "for c in cat:\n",
    "    for m in cat[c]:\n",
    "        mg[c]+=ms2[m]\n",
    "mg['mobility']/=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2={'macro':['macro_table','macro_sets'],\n",
    "    'industry':['matrix_label']+['matrix_HU']*3+['county_HU']*3,\n",
    "    'mobility':['county3ad_HU']*3*2,\n",
    "    'imobiliare':['county4c_202012HU']*3*4+['county4b_202012HU']*3*2+['county4_202012HU']*3*2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat2:\n",
    "    for m in cat2[c]:\n",
    "        mg[c]+=len(json.loads(open(htmlipath+'panels/'+m+'.json','r').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "cat2['status']=len([name for name in os.listdir(htmlipath+'panels/daily/')])*len(json.loads(open(htmlipath+'panels/daily/county2b_2021-01-01_HU.json','r').read()))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'panels/cats.json','w').write(json.dumps(list(\\\n",
    "     pd.DataFrame(mg,index=['count']).T.join(szotardf[languages]).T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(mg,index=['count']).T.join(szotardf[languages]).set_index('count').stack()\\\n",
    "                                                              .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2052"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(htmlipath+'panels/cats2.json','w').write(json.dumps(list(\\\n",
    "     df.T.to_dict().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']=pd.to_datetime('2020-04-05')\n",
    "df=df.set_index('date')\n",
    "df.columns=['count','lang','langtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging cats ...\n",
      "Writing to cats ...\n",
      "30 data points will be written in 0.006 batches.\n",
      "Expected query running time is: 3 seconds.\n",
      "Writing batch 1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "field_columns=['count']\n",
    "tag_columns=['lang','type','langtype']\n",
    "measurement='cats'\n",
    "push2influx(df,measurement,field_columns,tag_columns,fo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495509.0"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mg.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-29 02:28:20.512068\n"
     ]
    }
   ],
   "source": [
    "print(pd.to_datetime('now'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}